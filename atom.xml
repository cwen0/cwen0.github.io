<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>cwen</title>
  <id>http://int64.me</id>
  <updated>2018-04-08T10:36:05+08:00</updated>
  <subtitle>Life is magic. Coding is art.</subtitle>
  <link href="http://int64.me"></link>
  <entry>
    <title>TiDB 性能优化之玄学调优</title>
    <updated>2018-04-08T00:00:00Z</updated>
    <id>tag:int64.me,2018-04-08:/2018/TiDB 性能优化之玄学调优.html</id>
    <content type="html">&lt;p&gt;TiDB 玄学调优是我们自黑比较狠的梗之一，当初 TiKV 里大把的参数让我头疼了好久 ( TiKV 的配置参数大多数是和 RocksDB 相关 )。好在现在很多参数现在的默认值已经很科学大大的减少了我们玄学调优的难度。&#xA;一下是是我平时在测试 TiDB 时候，进行瓶颈定位的过程&amp;hellip;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;我们以 sysbench 测试为例，使用的测试脚本：&lt;a href=&#34;https://github.com/pingcap/tidb-bench/tree/master/sysbench&#34;&gt;https://github.com/pingcap/tidb-bench/tree/master/sysbench&lt;/a&gt;&#xA;可用机器公共四台，机器配置：CPU 16 核 / Mem 110 GB  / Disk 1024GB SSD&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;硬件性能测试&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;测试之前先进行基本的硬件测试，比如磁盘 fio 测试，网络延迟以及网速的测试，这些测试有利于排除一些干扰因素。&#xA;这地方就提供几本的测试方法，之前已经测试过了，此处就不实际测试了。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;测试磁盘：推荐 fio （fio 使用需谨慎，不要直接写裸设备，笔者连续写坏好几块盘，都是血泪啊），dd，sysbench&amp;hellip;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;// fio&#xA;fio -ioengine=libaio -bs=32k -direct=1 -thread -rw=read  -size=10G -filename=test -name=&amp;quot;PingCAP max throughput&amp;quot; -iodepth=4 -runtime=60&#xA;&#xA;// dd&#xA;dd bs=4k count=250000 oflag=direct if=/dev/zero of=./dd_test.txt&#xA;&#xA;// sysbench&#xA;sysbench --test=fileio --file-num=16 --file-block-size=16384 --file-total-size=2G prepare&#xA;sysbench --test=fileio --file-num=16 --file-block-size=16384 --file-total-size=2G --num-threads=4 --max-requests=100000000 --max-time=180 --file-test-mode=seqwr --file-extra-flags=direct run&#xA;sysbench --test=fileio --file-num=16 --file-block-size=16384 --file-total-size=2G --num-threads=4 --max-requests=100000000 --max-time=180 --file-test-mode=seqwr --file-extra-flags=direct clean&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;测试延迟：直接 ping 查看延迟以及丢包情况&lt;/p&gt;&#xA;&#xA;&lt;p&gt;测试网速: 最简单 dd + scp，dd + pv + nc , iperf&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;// scp&#xA;dd if=/dev/zero bs=2M count=2048 of=./test // 生成 4 GB 文件&#xA;scp ./test ip ip&#xA;&#xA;// dd + pv + nc&#xA;dd if=/dev/zero bs=1M count=1024 of=./test&#xA;pv test| nc 10.0.1.8 5433 // 发送&#xA;nc -vv -l -p 5433 &amp;gt; test  // 接收&#xA;&#xA;// iperf&#xA;iperf -s            // server&#xA;iperf -c 10.0.1.4   // client&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;sysbench oltp 测试瓶颈定位&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;运行 sysbench oltp 测试， sysbench 默认 oltp 一个事务包含 18 条 SQL:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;10 * point select&#xA;1 * simple range select&#xA;1 * sum range&#xA;1 * order range&#xA;1 * distinct range&#xA;1 * index update&#xA;1 * non index update&#xA;1 * insert&#xA;1 * delete&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;从事务的组合上来看度占大部分，经验告诉我们最先读多属于 CPU 密集型的操作，TiDB 的 CPU 最有可能成为瓶颈，我们先使用 1 TiDB + 3 TiKV 的架构进行测试，实际观察一下运行情况。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;使用 &lt;a href=&#34;https://github.com/pingcap/tidb-ansible&#34;&gt;tidb-ansible&lt;/a&gt; 部署集群&lt;/p&gt;&#xA;&#xA;&lt;p&gt;部署拓扑：&lt;/p&gt;&#xA;&#xA;&lt;table&gt;&#xA;&lt;thead&gt;&#xA;&lt;tr&gt;&#xA;&lt;th&gt;ip&lt;/th&gt;&#xA;&lt;th&gt;&lt;/th&gt;&#xA;&lt;/tr&gt;&#xA;&lt;/thead&gt;&#xA;&#xA;&lt;tbody&gt;&#xA;&lt;tr&gt;&#xA;&lt;td&gt;10.0.1.6&lt;/td&gt;&#xA;&lt;td&gt;tidb / pd / sysbench&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&#xA;&lt;tr&gt;&#xA;&lt;td&gt;10.0.1.7&lt;/td&gt;&#xA;&lt;td&gt;tikv&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&#xA;&lt;tr&gt;&#xA;&lt;td&gt;10.0.1.8&lt;/td&gt;&#xA;&lt;td&gt;tikv&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&#xA;&lt;tr&gt;&#xA;&lt;td&gt;10.0.1.9&lt;/td&gt;&#xA;&lt;td&gt;tikv&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;p&gt;测试流程：prepare -&amp;gt; run&lt;/p&gt;&#xA;&#xA;&lt;p&gt;运行测试脚本，先观察 tidb / tikv 的 cpu 使用情况（一般来说 oltp 测试，主要瓶颈会集中在 tidb 的 cpu 上，要又重点的去排查问题）&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;http://7xnp02.com1.z0.glb.clouddn.com/Screen%20Shot%202018-04-08%20at%2012.16.54%20AM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;可以看到 tidb 的机器的 load 已经达到了 20+，这是一台 15 核的机器，感觉这台机器的 cpu 几乎已经被榨干了，我们都不需要去看别的瓶颈，这 tidb 的cpu 已经限制了整体性能。那么我们接下来要做的就是要把这个瓶颈去掉，&lt;strong&gt;最直接的方法就是升级硬件或是加机器，我们尝试增加机器，同时主要注意增加压力&lt;/strong&gt;。（没有多余的机器就不做实验了） 当我们把 tidb 机器的数量也增加到 3 台或是更多的时候，我们就会发现瓶颈发生了转移，这个时候瓶颈就会从 tidb 转移到了 tikv，tikv 有个 end-point-concurrency 这样一个东西用来处理 tidb 的请求，我们使用 top -H 来观察一下&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;http://7xnp02.com1.z0.glb.clouddn.com/Screen%20Shot%202018-04-08%20at%2012.31.58%20AM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;对应 grafana 监控上的就是&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;http://7xnp02.com1.z0.glb.clouddn.com/Screen%20Shot%202018-04-08%20at%2012.34.02%20AM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;end-point-concurrency 什么时候算是出现瓶颈呢？ 默认 end-point-concurrency  是使用 4个线程，那么我们就可以这么认为，当 top -H 的时候发现这个四个线程的 cpu 使用都超过了 90% ，那么就可以认为 end-point-concurrency 限制了整个系统的吞吐，接下来我们要做的就是如何来把这个瓶颈给消除掉，如果目前 tikv 的机器整体 cpu 使用还是很闲，那么我们就&lt;strong&gt;增加这个 end-point-concurrency 的线程数量&lt;/strong&gt;，这个就要修改 tikv 的启动参数了,这个可以参考文档：&lt;a href=&#34;https://github.com/pingcap/docs-cn/blob/master/op-guide/tune-tikv.md&#34;&gt;https://github.com/pingcap/docs-cn/blob/master/op-guide/tune-tikv.md&lt;/a&gt; 。&#xA;（grpc 线程也是会出现瓶颈，改进方法和 end-point-concurrency 类似，直接增加线程数量）&lt;/p&gt;&#xA;&#xA;&lt;p&gt;当我们解决掉 end-point-concurrency，我们会发现系统的整体吞吐会有所提高，但是此时又会出现新的瓶颈，以我往常的经验来说，接下来的瓶颈应该会出现在  end-point-work 这个线程上，为嘛会出现在这个线程上呢？其实要是知道这个线程是用来干嘛的就很好理解了，这个现场是用来调度 end-point-concurrency 这个玩意掉，当 end-point-concurrency 线程多了，那么调度就会出现瓶颈，遗憾的是 end-point-work 是单线程，我们要解决这个的瓶颈，&lt;strong&gt;只能提升单核 cpu 的能力，或是增加 tikv 的数量&lt;/strong&gt;。&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Summary&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;瓶颈&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;TiDB CPU 一般会优先成为瓶颈&lt;/li&gt;&#xA;&lt;li&gt;TiKV 的 endpoint 线程&lt;/li&gt;&#xA;&lt;li&gt;grpc 线程&lt;/li&gt;&#xA;&lt;li&gt;TiKV 的 end-point-work 线程&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;优化&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;加 TiDB 节点，可以尝试在 TiKV 机器上添加（ TiKV CPU比较充足）&lt;/li&gt;&#xA;&lt;li&gt;调整 end-point-concurrency 参数，增大 endpoint 线程数量&lt;/li&gt;&#xA;&lt;li&gt;调整 grpc-concurrency 参数，增加 grpc 线程数量&lt;/li&gt;&#xA;&lt;li&gt;调大 racksdb cache&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;sysbench oltp 性能瓶颈定位就说这些，实际情况中 tidb 的瓶颈并不是一成不变的，需要根据实际情况去优化调整，要不然就不叫玄学了😄。&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;sysbench insert 测试瓶颈定位&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;sysbench insert 脚本很简单，就是顺序写，那么我们先从 tikv 基本概念考虑，tikv 内的最小几本单元是 region, 并且是每个表肯定是对应这个不同的 region，那么加入我们的测试只正对一个表 insert，我们有再多的 tikv 也是没有卵用，（对这个问题，目前有相应的解决办法，具体我也不是太清楚）所以我们测试要保证表的数量一定要大于 tikv 节点的数量（当然越多越好了），这里实验的时候我是使用了 16个表，每个表预先 prepare 了 100w 的数据。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;运行 insert 脚本，同样我会先 top 看 tidb ／ tikv ／pd 的 cpu 使用情况。&#xA;我们会很容易的发现 insert 的时候我们 TiDB 的 CPU 并不会首先成为瓶颈（并不绝对），但是发现 TiKV 的压力还是很大的，我们就需要去定位 TiKV 的瓶颈，如果 TiKV 压力也是很小，那么我们就要看一下是不是磁盘 IO 存在问题，查看磁盘 IO ，正常来说查看磁盘情况我使用 iostat&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;http://7xnp02.com1.z0.glb.clouddn.com/Screen%20Shot%202018-04-08%20at%209.58.10%20AM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;我们可以看到 sde 盘的 %util 在 70，也就是说挺忙的，但是还可以还没有达到瓶颈，正常来说超过了 90 以上，就可以认为这块盘已经被我们压榨的差不多了。&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;划水了，insert 测试的详细定位过程后面在单独写一篇吧&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;h3&gt;Summary&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;瓶颈&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;TiKV raftstore 线程&lt;/li&gt;&#xA;&lt;li&gt;磁盘 IO ，出现 stall 或是出现很多 IO wait&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;优化&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;如果在磁盘没有达到瓶颈的前提下，TiKV 机器的 CPU 还很富裕，可以单台多部署，一般单个 TiKV 实例对应一块盘&lt;/li&gt;&#xA;&lt;li&gt;磁盘 IO 出现瓶颈，可以调整压缩方式，用 CPU 资源换取 IO 资源&lt;/li&gt;&#xA;&lt;li&gt;发现系统的 IO 压力不大，但是 CPU资源已经吃光了，top -H 发现有大量的 bg 开头的线程（RocksDB 的 compaction 线程）在运行，这个时候可以考虑减少压缩用 IO 资源换取 CPU 资源&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;一点小提示&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Sysbench 版本建议使用 1.0 及以上版本&lt;/li&gt;&#xA;&lt;li&gt;当集群 TiKV 节点数量大于 3 的时候，在 prepare 数据完成后，需要等待 TiKV 节点上的 * leader 以及 region 数量均匀分布后，在进行各种基准测试，可以通过pd-ctl 或是 &lt;a href=&#34;http://hosts:port/pd/api/v1/stores&#34;&gt;http://hosts:port/pd/api/v1/stores&lt;/a&gt; 或是 Grafana 监控查看各个 TiKV 节点上 leader 和 region 的分布情况。&lt;/li&gt;&#xA;&lt;li&gt;当进行多轮测试的时候，当需要清理数据的时候，不要执行 DROP 操作，可以使用 Ansible 进行物理删除，然后在重启集群，重新 prepare 数据进行测试。&lt;/li&gt;&#xA;&lt;li&gt;当 TiDB , TiKV 都没有达到瓶颈的时候, 可以不断增大 Sysbench 线程数量&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;最后说一点&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;以上只是一个小的示例，在进行实际性能测试的过程中，总是会出现各种各样的情况，出现的瓶颈的地方也不是一成不变的，我所谓的玄学并不是没有根据的胡乱调整，而是分析测试实际情况加上自己对 TiDB 系统的理解，进行合理调整部署拓扑以及配置参数。当然如果对 TiDB / TiKV / PD 程序的优化感兴趣，我们也可以很好的利用压测 + Grafan 监控 + Perf + &lt;a href=&#34;https://github.com/brendangregg/FlameGraph&#34;&gt;FlameGraph&lt;/a&gt; 定位程序内部的瓶颈。&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;参考&lt;/h2&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://pingcap.com/blog-cn/tangliu-tool-1/&#34;&gt;工欲性能调优，必先利其器（1）&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://pingcap.com/blog-cn/tangliu-tool-2/&#34;&gt;工欲性能调优，必先利其器（2）- 火焰图&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://pingcap.com/blog-cn/tidb-internal-1/&#34;&gt;三篇文章了解 TiDB 技术内幕 - 说存储&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://int64.me/2017/sysbench%E7%9A%84%E4%B8%80%E7%82%B9%E6%95%B4%E7%90%86.html&#34;&gt;sysbench 的一点整理&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/pingcap/docs-cn/blob/master/op-guide/tune-tikv.md&#34;&gt;TiKV 性能参数调优&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;</content>
    <link href="http://int64.me/2018/TiDB 性能优化之玄学调优.html"></link>
    <author>
      <name>cwen</name>
    </author>
  </entry>
  <entry>
    <title>Percolator 论文笔记</title>
    <updated>2018-02-28T00:00:00Z</updated>
    <id>tag:int64.me,2018-02-28:/2018/Percolator 论文笔记.html</id>
    <content type="html">&lt;p&gt;TiDB 的事务模型是参考 Google Percolator 事务模型，想去研究 TiDB 的事务模型，学习一下 Google Percolator 论文必不可少&amp;hellip;&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Percolator 简介&lt;/h2&gt;&#xA;&#xA;&lt;h3&gt;背景&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;简单来说 Percolator 事务模型的出现是因为之前采用 MapReduce 设计的批量创建索引的系统不支持跨行事务以及不支持随机增量更新。于是乎 Percolator 就诞生了（ Google 还真是想啥有啥啊，实力真不是盖的 ）。&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;特性&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Percolator的特点如下&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;为增量处理定制&lt;/li&gt;&#xA;&lt;li&gt;处理结果强一致&lt;/li&gt;&#xA;&lt;li&gt;针对大数据量（小数据量用传统的数据库即可）&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Percolator 为可扩展的增量处理提供了两个主要抽象：&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;基于随机存取库的ACID事务&lt;/li&gt;&#xA;&lt;li&gt;观察者(observers)–一种用于处理增量计算的方式&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h3&gt;延迟问题&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;由于 Percolator 在设计的时候追求的是大数据量而对延迟没有特定要求，所有基于 Percolator 事物模型设计的系统，延迟和传统 DBMS 等数据库系统相比还是有很大的差距的。主要原因还是锁的问题。&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Percolator 使用一个 lazy 的方法去清理遗留下来的锁，比如一个 transactions 由于运行的机器挂掉了，这个 transactions 失败遗留下来的锁可能不会立刻清理，而是等待下一个使用到该数据的饿事务负责清理&lt;/li&gt;&#xA;&lt;li&gt;缺少全局 deadlock 检查，这样就使交易冲突的情况增多，事务重试的次数就会变多，延迟就会随之增大。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;设计&lt;/h2&gt;&#xA;&#xA;&lt;h3&gt;Bigtable 概述&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Percolator 是建立在 Bigtable 分布式系统的上层，Bigtable&#xA;是一个稀疏的、分布式的、持久化存储的多维度排序 Map。Map 的索引是行关键字、列关键字以及时间戳，Bigtable 以 SSTable 格式存储数据，并且这些 SSTables 是被存储在 GFS 上。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;http://7xnp02.com1.z0.glb.clouddn.com/Screen%20Shot%202018-02-27%20at%2012.44.16%20AM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;事务&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Percolator 提供了跨行、跨表的、基于快照隔离的ACID事务。&lt;/p&gt;&#xA;&#xA;&lt;h4&gt;Snapshop isolation&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;http://7xnp02.com1.z0.glb.clouddn.com/Screen%20Shot%202018-02-27%20at%209.34.11%20AM.png&#34; alt=&#34;sanpshop isolation&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;如图，是基于 Snapshop isolation  的三个 transactions，每个 Transaction 都是开始与 a start timestamp, 并且结束与 a commit timestamp。在这个例子中：&#xA;* 因为 transaction 2 的 start timestamp 是在transaction 1 commit timestamp 之前的，所以 transaction 2 不能够读到 transaction 1 提交的信息&#xA;* transaction 3 可以同时看到 transaction 1 和 transaction 2 的提交信息&#xA;* transaction 1 和 transaction 2 是并发的执行，如果他们写入同一项，这两个事务中至少又一个会执行失败&lt;/p&gt;&#xA;&#xA;&lt;h4&gt;Lock&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;Bigtable 本身并没有提供便捷的冲突解决和锁管理，Percolator 必须自己维护一套独立的锁的管理机制。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;要求：&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;容错，当机器出现故障的时候不影响正确性，如果在两阶段提交的过程中锁出现了丢失，可能将两个有冲突的事物都提交成功。&lt;/li&gt;&#xA;&lt;li&gt;高吞吐，上千台机器会同时请求获取锁。&lt;/li&gt;&#xA;&lt;li&gt;低延迟, 每个 Get() 操作都需要读取一次锁。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Percolator 需要做到：&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;多副本，容错&lt;/li&gt;&#xA;&lt;li&gt;分布式以及负载均衡，处理负载&lt;/li&gt;&#xA;&lt;li&gt;数据持久化&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;BigTable 能够满足以上所有要求。所以 Percolator 在实现时，将实际的数据存于Bigtable中。&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Columns in Bigtable&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;http://7xnp02.com1.z0.glb.clouddn.com/Screen%20Shot%202018-02-27%20at%2011.32.55%20PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Percolator 在 Bigtable 上抽象了 5 Columns 去存储数据，如上图，其中有 3 和事务相关：&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;c:lock&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;事务产生的锁，未提交的事务会写本项，会包含 primary lock 的位置。其映射关系为&lt;/p&gt;&#xA;&#xA;&lt;p&gt;${key,start_ts}=&amp;gt;${primary_key,lock_type,..etc}&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;${key} 数据的key&#xA;${start_ts} 事务开始时间&#xA;${primary} 该事务的primary的引用. primary是在事务执行时，从待修改的 keys中 选择一个作为primary,其余的则作为secondary.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;c:write&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;已提交的数据信息，存储数据所对应的时间戳。其映射关系为&lt;/p&gt;&#xA;&#xA;&lt;p&gt;${key,commit_ts}=&amp;gt;${start_ts}&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;${key} 数据的key&#xA;${commit_ts} 事务的提交时间&#xA;${start_ts} 该事务的开始时间,指向该数据在data中的实际存储位置。&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;c:data&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;具体存储数据集，映射关系为&lt;/p&gt;&#xA;&#xA;&lt;p&gt;${key,start_ts} =&amp;gt; ${value}&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;${key} 真实的key&#xA;${start_ts} 对应事务的开始时间&#xA;${value} 真实的数据值&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h3&gt;简述 Percolator 事务流程&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;事务提交采用两阶段提交。两阶段提交通过 client 来协调。&lt;/p&gt;&#xA;&#xA;&lt;h4&gt;Prewrite&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;http://7xnp02.com1.z0.glb.clouddn.com/prewrite.jpg&#34; alt=&#34;prewrite&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;图片 copy &lt;a href=&#34;http://andremouche.github.io/transaction/percolator.html&#34;&gt;雪姐 blog: Google Percolator 的事务模型&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;在提交的第一个阶段：其从 Oracle 获取代表当前物理时间的全局唯一时间戳作为当前事务的 start_ts，我们首先获取所有需要写入的 cell 的 lock（考虑到 client 故障情况，我们会任意指定一个 lock 为 primary，其余的作为 secondary），每个事务都会读取 meta 数据来检测事务是否存在冲突。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;有两种冲突的情况：&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;如果一个事务 A 看到 cell 的 write 列中已经存在一条记录，记录的时间戳晚于该事务开始的时间戳，那么说明存在写冲突，即说明有一个事务在 A 发起之后更新了 cell 的值，事务 A 需要 aborted。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;如果事务看到 cell 的 lock 列中存在任意一条锁记录，不管时间戳为多少，直接 aborted。&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;如果两种冲突都不存在，向 lock 列中写入上锁信息，并更新 data 列数据。&lt;/p&gt;&#xA;&#xA;&lt;h4&gt;Commit&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;http://7xnp02.com1.z0.glb.clouddn.com/commit.jpg&#34; alt=&#34;commit&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;图片 copy &lt;a href=&#34;http://andremouche.github.io/transaction/percolator.html&#34;&gt;雪姐 blog: Google Percolator 的事务模型&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;如果没有 cell 冲突，那么说明事务可以提交，进行下一个阶段 commit：首先 client 会向时间服务器申请一个 timestamp 作为 commit_ts，表示 commit 的时间。然后 client 会释放事务中涉及的所有 cell 的锁（清空 lock 列），释放顺序从 primary lock开始。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;释放锁之后便更新 write 列来使新的 read 能够读到本次事务的数据。write 列的数据表示：本次事务的数据已经成功更新至 cell 中，write 列中的数据包含了一个 timestamp，该 timestamp 表示本次事务数据的 timestamp，用户可以通过该 timestamp 来找到数据。一旦 primary 锁对应记录的 write 列数据可见了，代表该事务一定已经 commit 了，reader 此时能看到该事务的数据。&lt;/p&gt;&#xA;&#xA;&lt;h4&gt;Get operation&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;http://7xnp02.com1.z0.glb.clouddn.com/get.jpg&#34; alt=&#34;get&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;图片 copy &lt;a href=&#34;http://andremouche.github.io/transaction/percolator.html&#34;&gt;雪姐 blog: Google Percolator 的事务模型&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Get 操作首先检查[0,start_ts]时间区间内Lock是否存在，若存在，说明可能有其他 transaction 正在写这个 cell，所以这个读 transaction 需要等待 lock 释放掉。如果不存在有冲突的 lock,则获取在 write 中合法的最新提交记录指向的在 data 中的位置。&lt;/li&gt;&#xA;&lt;li&gt;从 data 中获取到相应的数据并返回。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h4&gt;clean lock&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;http://7xnp02.com1.z0.glb.clouddn.com/rollback.jpg&#34; alt=&#34;clean lock&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;图片 copy &lt;a href=&#34;http://andremouche.github.io/transaction/percolator.html&#34;&gt;雪姐 blog: Google Percolator 的事务模型&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;若客户端在 Commit 一个事务时，出现了异常，Prepare 时产生的锁会被留下。为避免将新事务 hang 住，Percolator 必须清理这些锁。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Percolator 用 lazy 方式处理这些锁：当事务 A 在执行时，发现事务 B 造成的锁冲突，事务 A 将决定事务 B 是否失败，以及清理事务 B 的那些锁。&#xA;对事务 A 而言，能准确地判断事务 B 是否成功是关键。Percolator 为每个事务设计了一个元素cell作为事务是否成功的同步标准，该元素产生的 lock 即为 primary lock。A 和 B 事务都能确定事务 B 的 primary lock（因为这个priarmy lock被写入了B事务其它所有涉及元素的lock里面）。执行一个清理 clean up 或者提交 commit 操作需要修改该primary lock，由于这些修改是基于 Bigtable 去做，所以只有一个清理或提交会成功。注意：&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;在B提交 commit 之前,它会先确保其 primary lock 被 write record 所替代（即往 primary 的 write 写提交数据，并删除对应的 lock）。&lt;/li&gt;&#xA;&lt;li&gt;在 A 清理 B 的锁之前，A 必须检查 B 的 primary 以确保 B 未被提交，如果 B 的 primary 存在，则 B 的锁可以被安全的清理掉。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;当客户端在执行两阶段提交的 commit 阶段失败时，事务依旧会留下一个提交点 commit point(至少一条记录会被写入 write 中)，但可能会留下一些 lock 未被处理掉。一个事务能够从其 primary lock 中获取到执行情况：&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;如果 priarmy lock 已被 write 所替代，也就是说该事务已被提交，可以放心的清理 B 事务的所有 lock&lt;/li&gt;&#xA;&lt;li&gt;如果 primary lock 存在，事务被 roll back (因为我们总是最先提交 primary ,所以 primary 未被提交时，可以安全地执行回滚)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h3&gt;案例&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;http://7xnp02.com1.z0.glb.clouddn.com/Screen%20Shot%202018-02-28%20at%2012.59.40%20AM.png&#34; alt=&#34;1&#34; /&gt;&#xA;&amp;gt; 初始状态下，Bob 的帐户下有 $10（首先查询 column write 获取最新时间戳数据,获取到 data@5,然后从 column data 里面获取时间戳为5的数据，即 $10），Joe 的帐户下有 $2。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;http://7xnp02.com1.z0.glb.clouddn.com/Screen%20Shot%202018-02-28%20at%2012.59.46%20AM.png&#34; alt=&#34;2&#34; /&gt;&#xA;&amp;gt; 转账开始，使用 stat timestamp=7 作为当前事务的开始时间戳，将 Bob 选为本事务的 primary，通过写入 Column Lock 锁定 Bob 的帐户，同时将数据 7:$3 写入到 Column,data 列。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;http://7xnp02.com1.z0.glb.clouddn.com/Screen%20Shot%202018-02-28%20at%2012.59.53%20AM.png&#34; alt=&#34;3&#34; /&gt;&#xA;&amp;gt; 同样的，使用 stat timestamp=7，锁定 Joe 的帐户，并将 Joe 改变后的余额写入到 Column,data ,当前锁作为 secondary 并存储一个指向 primary 的引用（当失败时，能够快速定位到 primary 锁，并根据其状态异步清理）&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;http://7xnp02.com1.z0.glb.clouddn.com/Screen%20Shot%202018-02-28%20at%201.00.00%20AM.png&#34; alt=&#34;4&#34; /&gt;&#xA;&amp;gt; 事务带着当前时间戳 commit timestamp=8 进入 commit 阶段：删除 primary 所在的 lock，并在 write 列中写入从提交时间戳指向数据存储的一个指针 commit_ts=&amp;gt;data @7。至此，读请求过来时将看到 Bob 的余额为3。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;http://7xnp02.com1.z0.glb.clouddn.com/Screen%20Shot%202018-02-28%20at%201.00.06%20AM.png&#34; alt=&#34;5&#34; /&gt;&#xA;&amp;gt; 依次在 secondary 项中写入wirte并清理锁，整个事务提交结束。&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;参考&lt;/h2&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.usenix.org/legacy/event/osdi10/tech/full_papers/Peng.pdf&#34;&gt;Large-scale Incremental Processing&#xA;Using Distributed Transactions and Notifications&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://andremouche.github.io/transaction/percolator.html&#34;&gt;雪姐 blog: Google Percolator 的事务模型&lt;/a&gt; - 很多描述是从雪姐 blog 直接 copy 过来&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.jianshu.com/p/05194f4b29dd&#34;&gt;Percolator简单翻译与个人理解&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;</content>
    <link href="http://int64.me/2018/Percolator 论文笔记.html"></link>
    <author>
      <name>cwen</name>
    </author>
  </entry>
  <entry>
    <title>MVCC In TiKV</title>
    <updated>2017-11-28T00:00:00Z</updated>
    <id>tag:int64.me,2017-11-28:/2017/MVCC In TiKV.html</id>
    <content type="html">&lt;p&gt;很多数据库都会实现多版本控制（MVCC），TiKV 也不例外，刚好最近在看 TiKV，对于 MVCC 以及 TiKV 内是如何使用 MVCC 的做个简单笔记&amp;hellip;&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;乐观锁和悲观锁&lt;/h2&gt;&#xA;&#xA;&lt;h3&gt;乐观锁&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;乐观锁呢，读写事务，在真正的提交之前，不加读/写锁，而是先看一下数据的版本/时间戳，等到真正提交的时候再看一下版本/时间戳，如果两次相同，说明别人期间没有对数据进行过修改，那么就可以放心提交。乐观体现在，访问数据时不提前加锁。在资源冲突不激烈的场合，用乐观锁性能较好。如果资源冲突严重，乐观锁的实现会导致事务提交的时候经常看到别人在他之前已经修改了数据，然后要进行回滚或者重试，还不如一上来就加锁。&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;悲观锁&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;一个读写事务在运行的过程中在访问数据之前先加读/写锁这种实现叫做悲观锁，悲观体现在，先加锁，独占数据，防止别人加锁。&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;关于乐观锁悲观锁的解释 copy 自：&lt;a href=&#34;https://www.zhihu.com/question/27876575/answer/73330077&#34;&gt;吴镝大神知乎的回答&lt;/a&gt;, 感觉回答的最简洁贴切&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;h2&gt;并发控制&lt;/h2&gt;&#xA;&#xA;&lt;h3&gt;可串行化&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;多个事务的并发执行是正确的，当且仅当其结果与按某一次序串行地执行它们时的结果相同。我们称这种调度策略为可串行化（Serializable）的调度。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;可串行性是并发事务正确性的准则。按这个准则规定，一个给定的并发调度，当且仅当它是可串行化的，才认为是正确调度。DBMS的并发控制机制必须提供一定的手段来保证调度是可串行化的，目前DBMS普遍采用封锁方法实现并发操作调度的可串行性，从而保证调度的正确性。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;两段锁（Two-Phase Locking，简称2PL）协议就是保证并发调度可串行性的封锁协议。&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;2PL&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;两段锁（Two-Phase Locking，简称2PL）是指所有事务都必须分为两阶段对数据进行加锁和解锁：&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;对任何数据进行读、写操作之前，首先要先申请并获得对该数据的封锁&lt;/li&gt;&#xA;&lt;li&gt;在释放一个封锁以后，事务不再获得任何其他封锁&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;在“两段”锁协议中，事务分为两个阶段：&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;第一阶段是获得封锁，也称为扩展阶段。这在阶段，事务可以申请获得任何数据项上的任何类型的锁，但是不能释放任何锁。&lt;/li&gt;&#xA;&lt;li&gt;第二阶段是释放封锁，也称为收缩阶段。在这阶段，事务可以释放任何数据项上的任何类型的琐，但是不能再申请任何琐。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;如图：&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;http://7xnp02.com1.z0.glb.clouddn.com/Screen%20Shot%202017-11-27%20at%2011.44.23%20PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;2PL 一些缺点&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;读锁和写锁会相互阻滞（block）。&lt;/li&gt;&#xA;&lt;li&gt;大部分事务都是只读（read-only）的，所以从事务序列（transaction-ordering）的角度来看是无害的。如果使用基于锁的隔离机制，而且如果有一段很长的读事务的话，在这段时间内这个对象就无法被改写，后面的事务就会被阻塞直到这个事务完成。这种机制对于并发性能来说影响很大。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h3&gt;MVCC&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;MVCC - 多版本并发控制（Multi-Version Concurrency Control）, 在 MVCC 中，每当想要更改或者删除某个数据对象时，DBMS 不会在原地去删除或这修改这个已有的数据对象本身，而是创建一个该数据对象的新的版本，这样的话同时并发的读取操作仍旧可以读取老版本的数据，而写操作就可以同时进行。这个模式的好处在于，可以让读取操作不再阻塞，事实上根本就不需要锁。这是一种非常诱人的特型，以至于在很多主流的数据库中都采用了 MVCC 的实现，比如说 PostgreSQL，Oracle，Microsoft SQL Server 等。&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;此处 copy 自&lt;a href=&#34;https://pingcap.com/blog-cn/MVCC-in-TiKV/&#34;&gt;TiKV 的 MVCC（Multi-Version Concurrency Control）机制&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;h2&gt;MVCC in TiKV&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;TiKV 目前的底层存储还是使用了 rocksdb， 也就是目前我们的所有数据也就是存在 rocksdb 内。目前 TiKV 会启动两个 rocksdb 实例，默认 rocksdb 实例将 KV 数据存储在内部的 default、write 和 lock 3 个 CF 内。&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;default CF 存储的是真正的数据；&lt;/li&gt;&#xA;&lt;li&gt;write CF 存储的是数据的版本信息（MVCC）以及索引相关的数据；&lt;/li&gt;&#xA;&lt;li&gt;lock CF 存储的是锁信息。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Raft RocksDB 实例存储 Raft log。&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;default CF 主要存储的是 raft log。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;rocksdb 的 cf 是一个逻辑划分数据库的能力，也就是说做到了想多隔离的存储，但是 rocksdb 提供跨 cf 的原子写操作，不同的 cf 共用同一个 WAL，但是使用不同 memtable 和 ssl。（具体 rocksdb 相关的知识可以查阅 rocksdb 相关文档）。也就是说会有一个单独的 cf 用来存储 MVCC 的版本信息。 TiKV 的 MVCC 实现是通过在 Key 后面添加 Version 来实现，简单来说，没有 MVCC 之前，可以把 TiKV 看做这样的：&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;                Key1 -&amp;gt; Value&#xA;&#x9;            Key2 -&amp;gt; Value&#xA;&#x9;            ……&#xA;&#x9;            KeyN -&amp;gt; Value&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;有了 MVCC 之后，TiKV 的 Key 排列是这样的：&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&#x9;            Key1-Version3 -&amp;gt; Value&#xA;&#x9;            Key1-Version2 -&amp;gt; Value&#xA;&#x9;            Key1-Version1 -&amp;gt; Value&#xA;&#x9;            ……&#xA;&#x9;            Key2-Version4 -&amp;gt; Value&#xA;&#x9;            Key2-Version3 -&amp;gt; Value&#xA;&#x9;            Key2-Version2 -&amp;gt; Value&#xA;&#x9;            Key2-Version1 -&amp;gt; Value&#xA;&#x9;            ……&#xA;&#x9;            KeyN-Version2 -&amp;gt; Value&#xA;&#x9;            KeyN-Version1 -&amp;gt; Value&#xA;&#x9;            ……&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;注意，对于同一个 Key 的多个版本，我们把版本号较大的放在前面，版本号小的放在后面，这样当用户通过一个 Key + Version 来获取 Value 的时候，可以将 Key 和 Version 构造出 MVCC 的 Key，也就是 Key-Version。然后可以直接 Seek(Key-Version)，定位到第一个大于等于这个 Key-Version 的位置。&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;此处 copy 自申栎哥文章&lt;a href=&#34;https://pingcap.com/blog-cn/tidb-internal-1/&#34;&gt;三篇文章了解 TiDB 技术内幕 - 说存储&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;现在从代码看起：&lt;/p&gt;&#xA;&#xA;&lt;p&gt;我们来看 TiKV 的 Storage pkg，可以看到这个 pkg 里面有个 MVCC pkg，没错具体的 MVCC 操作实现就是定义在 MVCC 这个 pkg 里面。&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;先看 Storage&lt;/h3&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;pub struct Storage {&#xA;    engine: Box&amp;lt;Engine&amp;gt;,&#xA;    sendch: SyncSendCh&amp;lt;Msg&amp;gt;,&#xA;    handle: Arc&amp;lt;Mutex&amp;lt;StorageHandle&amp;gt;&amp;gt;,&#xA;    ...&#xA;}&#xA;impl Storage {&#xA; pub fn start(&amp;amp;mut self, config: &amp;amp;Config) -&amp;gt; Result&amp;lt;()&amp;gt; {&#xA;        let mut handle = self.handle.lock().unwrap();&#xA;        if handle.handle.is_some() {&#xA;            return Err(box_err!(&amp;quot;scheduler is already running&amp;quot;));&#xA;        }&#xA;&#xA;        let engine = self.engine.clone();&#xA;        let builder = thread::Builder::new().name(thd_name!(&amp;quot;storage-scheduler&amp;quot;));&#xA;        let rx = handle.receiver.take().unwrap();&#xA;        let sched_concurrency = config.scheduler_concurrency;&#xA;        let sched_worker_pool_size = config.scheduler_worker_pool_size;&#xA;        let sched_pending_write_threshold = config.scheduler_pending_write_threshold.0 as usize;&#xA;        let ch = self.sendch.clone();&#xA;        let h = builder.spawn(move || {&#xA;            let mut sched = Scheduler::new(&#xA;                engine,&#xA;                ch,&#xA;                sched_concurrency,&#xA;                sched_worker_pool_size,&#xA;                sched_pending_write_threshold,&#xA;            );&#xA;            if let Err(e) = sched.run(rx) {&#xA;                panic!(&amp;quot;scheduler run err:{:?}&amp;quot;, e);&#xA;            }&#xA;            info!(&amp;quot;scheduler stopped&amp;quot;);&#xA;        })?;&#xA;        handle.handle = Some(h);&#xA;&#xA;        Ok(())&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;其实 Storage 是实际接受外部指令, Storage 内包含三个字段：&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Engine，数据库操作的接口，raftkv 以及 rocksdb 实现了这个接口，具体实现可以看 engine/raftkv.rs, engine/rocksdb.rs&lt;/li&gt;&#xA;&lt;li&gt;SyncSendCh, 一个 channel 内部类型是 Msg, 用来存储 scheduler event 的 channel&lt;/li&gt;&#xA;&lt;li&gt;StorageHanle, 是处理从sench 接受到指令，通过 mio 来处理 IO&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;可以看到 Storage 最后启动了调度器，然后不断的接受客户端指令，然后在传给 scheduler, 然后调度器执行相应的过程或者调用相应的异步函数。在调度器中有两种操作类型，读和写。&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;MVCC MVCCReader&lt;/h3&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;pub struct MVCCReader {&#xA; ....&#xA;}&#xA;&#xA;impl MVCCReader{&#xA;    pub fn new() {...};&#xA;    pub fn get_statistics(&amp;amp;self) -&amp;gt; &amp;amp;Statistics {...}&#xA;    pub fn set_key_only(&amp;amp;mut self, key_only: bool) {...}&#xA;    pub fn load_data(&amp;amp;mut self, key: &amp;amp;Key, ts: u64) -&amp;gt; Result&amp;lt;Value&amp;gt; {...}&#xA;    pub fn load_lock(&amp;amp;mut self, key: &amp;amp;Key) -&amp;gt; Result&amp;lt;Option&amp;lt;Lock&amp;gt;&amp;gt; {...}&#xA;    pub fn get(&amp;amp;mut self, key: &amp;amp;Key, mut ts: u64) -&amp;gt; Result&amp;lt;Option&amp;lt;Value&amp;gt;&amp;gt; {...}&#xA;    pub fn get_txn_commit_info(&#xA;        &amp;amp;mut self,&#xA;        key: &amp;amp;Key,&#xA;        start_ts: u64,&#xA;    ) -&amp;gt; Result&amp;lt;Option&amp;lt;(u64, WriteType)&amp;gt;&amp;gt; {...}&#xA;    pub fn seek_ts(&amp;amp;mut self, ts: u64) -&amp;gt; Result&amp;lt;Option&amp;lt;Key&amp;gt;&amp;gt; {...}&#xA;    pub fn seek(&amp;amp;mut self, mut key: Key, ts: u64) -&amp;gt; Result&amp;lt;Option&amp;lt;(Key, Value)&amp;gt;&amp;gt; {...}&#xA;    pub fn reverse_seek(&amp;amp;mut self, mut key: Key, ts: u64) -&amp;gt; Result&amp;lt;Option&amp;lt;(Key, Value)&amp;gt;&amp;gt; {...}&#xA;    ...&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;看 MVCCReader 结构很容易理解，各种读的操作。&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;MVCCTxn&lt;/h3&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;pub struct MVCCTxn {&#xA;    reader: MVCCReader,&#xA;    start_ts: u64,&#xA;    writes: Vec&amp;lt;Modify&amp;gt;,&#xA;    write_size: usize,&#xA;}&#xA;impl MVCCTxn {&#xA;    pub fn prewrite(&#xA;        &amp;amp;mut self,&#xA;        mutation: Mutation,&#xA;        primary: &amp;amp;[u8],&#xA;        options: &amp;amp;Options,&#xA;    ) -&amp;gt; Result&amp;lt;()&amp;gt; {...}&#xA;&#xA;    pub fn commit(&amp;amp;mut self, key: &amp;amp;Key, commit_ts: u64) -&amp;gt; Result&amp;lt;()&amp;gt; {...}&#xA;    pub fn rollback(&amp;amp;mut self, key: &amp;amp;Key) -&amp;gt; Result&amp;lt;()&amp;gt; {...}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;MVCCTxn 实现了两段提交（2-Phase Commit，2PC），整个 TiKV 事务模型的核心。在一段事务中，由两个阶段组成。&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Prewrite&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;选择一个 row 作为 primary row， 余下的作为 secondary row。 对primary row 上锁. 在上锁之前，会检查是否有其他同步的锁已经上到了这个 row 上 或者是是否经有在 startTS 之后的提交操作。这两种情况都会导致冲突，一旦都冲突发生，就会回滚（rollback）。 对于 secondary row 重复以上操作。&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Commit&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Rollback 在Prewrite 过程中出现冲突的话就会被调用。&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Garbage Collector&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;很容易发现，如果没有垃圾收集器（Gabage Collector） 来移除无效的版本的话，数据库中就会存有越来越多的 MVCC 版本。但是我们又不能仅仅移除某个 safe point 之前的所有版本。因为对于某个 key 来说，有可能只存在一个版本，那么这个版本就必须被保存下来。在TiKV中，如果在 safe point 前存在Put 或者Delete，那么说明之后所有的 writes 都是可以被移除的，不然的话只有Delete，Rollback和Lock 会被删除。&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;此处部分 copy 自&lt;a href=&#34;https://pingcap.com/blog-cn/MVCC-in-TiKV/&#34;&gt;TiKV 的 MVCC（Multi-Version Concurrency Control）机制&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;h2&gt;参考&lt;/h2&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Two-phase_locking&#34;&gt;Two-phase locking&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.zhihu.com/question/60278698&#34;&gt;OCC和MVCC的区别是什么？&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://pingcap.com/blog-cn/tidb-internal-1/&#34;&gt;三篇文章了解 TiDB 技术内幕 - 说存储&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://pingcap.com/blog-cn/MVCC-in-TiKV/&#34;&gt;TiKV 的 MVCC（Multi-Version Concurrency Control）机制&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;</content>
    <link href="http://int64.me/2017/MVCC In TiKV.html"></link>
    <author>
      <name>cwen</name>
    </author>
  </entry>
  <entry>
    <title>Spanner 论文笔记</title>
    <updated>2017-11-05T00:00:00Z</updated>
    <id>tag:int64.me,2017-11-05:/2017/Spanner 论文笔记.html</id>
    <content type="html">&lt;p&gt;刚到公司的时候，记得奇哥就推荐了几篇基础论文（尴尬，目前我还没有完全看完），Spanner 论文就是其中之一，同时 TiKV 的设计架构也是受到了 Spanner 的启发，在阅读 Spanner 论文的同时，记录些东西，方便以后回顾复习&amp;hellip;&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;结构&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;一套 Spanner 系统称为一个 universe, Spanner 论文上说目前 google 总共部署了三套  Spanner，一个开发，一个测试，一个线上。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;http://7xnp02.com1.z0.glb.clouddn.com/1752522-8d4d85a54036697f.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;上图是 Spanner 论文上的结构图，可以看出Spanner 是由多个 zone （管理部署的基本单元） 组成，一个数据中心中可以有多个 zone。 一个 zone 包含一个 zonemaster和一百或者几千个 spanserver。zonemaster 把数据分配给 spanserver，spanserver 把数据提供给客户端。客户端使用没个 zone 上的 location proxy 来定位提供服务的 spanserver，spanerver 提供具体的服务。（TiDB 的 PD 与 TiKV 之间的配合还真是有异曲同工之妙啊）。Universe master 主要是一个控制台，它显示了关于 zone 的各种状态信息，可以用于相互之间的调试。Placement driver 会周期性地与 spanserver 进行交互，来发现那些需要被转移的数据，或者是为了满足新的副本约束条件，或者是为了进行负载均衡。&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;spanserver 实现&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;http://7xnp02.com1.z0.glb.clouddn.com/1752522-b99c0dffe8361eb9.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;从图中可以看出，每个 spanserver 负载管理 100 - 1000 个称为 tablet 的数据结构的实例。一个 tablet 就类似于 BigTable 中的 tablet，也实现了下面的映射: &lt;code&gt;(key:string, timestamp:int64)-&amp;gt;string&lt;/code&gt; 。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;spanserver 不同于 bigtable 的地方在于，spanserver 其实更像一个完整关系数据库，而不是一个纯粹的键值存储，spanserver 会为每一个数据分配一个时间戳（有点类似 MVCC），tablet 的状态存储在类似于 B-树的文件集合以及 WAL（write-ahead-log)中，并且都存放在 GFS 的升级版 Colossus 文件系统之上。（感觉结构有点复杂啊，也可能是我先了解了 TiKV 的设计，有点先入为主了）。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;再说 tablet 的上层，从图中可以看到，是用 Paxos 状态机来做副本管理（Paxos 算饭没有仔细研究过，钥匙又地方描述的有问题，欢迎指正）。论文里面讲到，目前每个 tablet 上实现了一个 Paxos 状态机，每次写操作的时候都需要写两份数据，一份落到 tablet 的日志中，一份是写入，Paxos 的日志中，目前Paxos 的 leader 实现了支持长寿命的 leader ，简单来说就是使用了租约的机制（lease TiKV 中 Raft leader 也是如此）。写操作必须在领导者上初始化 Paxos 协议，读操作可以直接从底层的任何副本的 tablet 中访问状态信息，只要这个副本足够新。副本的集合被称为一个 Paxos group。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;对于每个是领导者的副本而言，每个 spanserver 会实现一个锁表来实现并发控制。这个锁表包含了两阶段锁机制的状态:它把键的值域映射到锁状态上面。注意，采用一个长寿命的 Paxos 领导者，对于有效管理锁表而言是非常关键的。在 BigTable 和 Spanner 中，我们都专门为长事务做了设计，比如，对于报表操作，可能要持续几分钟，当存在冲突时，采用乐观并发控制机制会表现出很差的性能。对于那些需要同步的操作，比如事务型的读操作，需要获得锁表中的锁，而其他类型的操作则可以不理会锁表。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;对于每个扮演领导者角色的副本，每个 spanserver 也会实施一个事务管理器来支持分布式事务。这个事务管理器被用来实现一个 participant leader，该组内的其他副本则是作为 participant slaves。如果一个事务只包含一个 Paxos 组(对于许多事务而言都是如此)，它就可以绕过事务管理器，因为锁表和 Paxos 二者一起可以保证事务性。如果一个事务包含了多 于一个 Paxos 组，那些组的领导者之间会彼此协调合作完成两阶段提交。其中一个参与者组，会被选为协调者，该组的 participant leader 被称为 coordinator leader，该组的 participant slaves 被称为 coordinator slaves。每个事务管理器的状态，会被保存到底层的 Paxos 组。目前 TiKV 的事务好像都走的是两阶段提交，不知到要是也搞个事务管理器效果会如何？&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;dictionary&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Spanner 对具有公共前缀的键做了一个抽象，称为 dictionary。目前一个 dictionary 是数据存放的基本单位。。属于一个目录的所有数据，都具有相同的副本配置。 当数据在不同的 Paxos 组之间进行移动时，会一个目录一个目录地转移，如下图所示。Spanner 可能会移动一个目录从而减轻一个 Paxos 组的负担，也可能会把那些被频繁地一起访问的目录都放置到同一个组中，或者会把一个目录转移到距离访问者更近的地方。当客户端操作正在进行时，也可以进行目录的转移。我们可以预期在几秒内转移 50MB 的目录。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;dictionary 是数据复制和placement配置的基本单位。spanner中负载均衡的最小单位也是dictionary，同时提供方法MoveDir可以手动将一个dictionary移动到指定的zone&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;http://7xnp02.com1.z0.glb.clouddn.com/1752522-192ba58099b34f11.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;数据模型&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;spanner的行模型是 &lt;code&gt;(key:string, timestamp:int64) -&amp;gt; row content&lt;/code&gt;，可以看到跟big table的模型最大的不同是这里强化了row的概念，不再突出column；这样spanner的timestamp是赋给整行数据的，是有物理意义的，这使得spanner更像一个实现多版本并发的数据库，而在big table中，timestamp仅仅用于保存多个版本的key-value，跟并发完全无关；我觉得这也是为什么spanner称自己为semi-relational 数据库，而big table只称自己是semi-structure 数据库的原因。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;http://7xnp02.com1.z0.glb.clouddn.com/1752522-c784a00321761682.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Spanner 的数据模型不是纯粹关系型的，它的行必须有名称。更准确地说，每个表都需 要有包含一个或多个主键列的排序集合。这种需求，让 Spanner 看起来仍然有点像键值存储: 主键形成了一个行的名称，每个表都定义了从主键列到非主键列的映射。当一个行存在时，必须要求已经给行的一些键定义了一些值(即使是 NULL)。采用这种结构是很有用的，因为这可以让应用通过选择键来控制数据的局部性。&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;TrueTime API&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;http://7xnp02.com1.z0.glb.clouddn.com/1752522-d1557f77c3323255.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;rueTime API 是一个非常有创意的东西，可以同步全球的时间。TT.now()可以获得一个绝对时间TTinterval，这个值和UnixTime是相同的，同时还能够得到一个误差e。TT.after(t)和TT.before(t)是基于TT.now()实现的。那这个TrueTime API实现靠的是GFS和原子钟。之所以要用两种技术来处理，是因为导致这两个技术的失败的原因是不同的。GPS会有一个天线，电波干扰会导致其失灵。原子钟很稳定。当GPS失灵的时候，原子钟仍然能保证在相当长的时间内，不会出现偏差。实际部署的时候。每个数据中心需要部署一些Master机器，其他机器上需要有一个slave进程来从Master同步。有的Master用GPS，有的Master用原子钟。&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;并发控制&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Spanner使用TrueTime来控制并发，实现外部一致性。支持以下几种事务。&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;读写事务&lt;/li&gt;&#xA;&lt;li&gt;只读事务&lt;/li&gt;&#xA;&lt;li&gt;快照读，客户端提供时间戳&lt;/li&gt;&#xA;&lt;li&gt;快照读，客户端提供时间范围&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;http://7xnp02.com1.z0.glb.clouddn.com/1752522-91fa02b85deaedff.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;上表是Spanner现在支持的事务。单独的写操作都被实现为读写事务 ； 单独的非快照被实现为只读事务。事务总有失败的时候，如果失败，对于这两种操作会自己重试，无需应用自己实现重试循环。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;时间戳的设计大大提高了只读事务的性能。事务开始的时候，要声明这个事务里没有写操作，只读事务可不是一个简单的没有写操作的读写事务。它会用一个系统时间戳去读，所以对于同时的其他的写操作是没有Block的。而且只读事务可以在任意一台已经更新过的replica上面读。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;对于快照读操作，可以读取以前的数据，需要客户端指定一个时间戳或者一个时间范围。Spanner会找到一个已经充分更新好的replica上读取。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;还有一个有趣的特性的是，对于只读事务，如果执行到一半，该replica出现了错误。客户端没有必要在本地缓存刚刚读过的时间，因为是根据时间戳读取的。只要再用刚刚的时间戳读取，就可以获得一样的结果。&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Paxos 领导者租约&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Spanner 的 Paxos 实现中使用了时间化的租约，来实现长时间的领导者地位(默认是 10秒)。一个潜在的领导者会发起请求，请求时间化的租约投票，在收到指定数量的投票后，这个领导者就可以确定自己拥有了一个租约。一个副本在成功完成一个写操作后，会隐式地延期自己的租约。对于一个领导者而言，如果它的租约快要到期了，就要显示地请求租约延期。另一个领导者的租约有个时间区间，这个时间区间的起点就是这个领导者获得指定数量的投票那一刻，时间区间的终点就是这个领导者失去指定数量的投票的那一刻(因为有些投票已经过期了)。Spanner 依赖于下面这些“不连贯性”:对于每个 Paxos 组，每个 Paxos 领 导者的租约时间区间，是和其他领导者的时间区间完全隔离的。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Spanner 实现允许一个 Paxos 领导者通过把 slave 从租约投票中释放出来这种方式，实现领导者的退位。为了保持这种彼此隔离的不连贯性，Spanner 会对什么时候退位做出限制。把 smax 定义为一个领导者可以使用的最大的时间戳。在退位之前，一个领导者必须等到 TT.after(smax)是真。&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;为读写事务分配时间戳&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;事务读和写采用两段锁协议。当所有的锁都已经获得以后，在任何锁被释放之前，就可以给事务分配时间戳。对于一个给定的事务，Spanner 会为事务分配时间戳，这个时间戳是 Paxos 分配给 Paxos 写操作的，它代表了事务提交的时间。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Spanner 依赖下面这些单调性:在每个 Paxos 组内，Spanner 会以单调增加的顺序给每个 Paxos 写操作分配时间戳，即使在跨越多个领导者时也是如此。一个单个的领导者副本，可以很容易地以单调增加的方式分配时间戳。在多个领导者之间就会强制实现彼此隔离的不连 贯:一个领导者必须只能分配属于它自己租约时间区间内的时间戳。要注意到，一旦一个时间戳 s 被分配，smax 就会被增加到 s，从而保证彼此隔离性(不连贯性)。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Spanner 也会实现下面的外部一致性:如果一个事务 T2 在事务 T1 提交以后开始执行， 那么，事务 T2 的时间戳一定比事务 T1 的时间戳大。对于一个事务 Ti 而言，定义开始和提交事件eistart和eicommit，事务提交时间为si。对外部一致性的要求就变成了:&#xA;tabs(e1commit )&lt;tabs(e2start ) s1&lt;s2。执行事务的协议和分配时间戳的协议，遵守两条规则，二者一起保证外部一致性。对于一个写操作 Ti 而言，担任协调者的领导者发出的提交请求的事件为eiserver 。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Start. 为一个事务 Ti 担任协调者的领导者分配一个提交时间戳 si，不会小于 TT.now().latest 的值，TT.now().latest的值是在esierver事件之后计算得到的。要注意，担任参与者的领导者， 在这里不起作用。第 4.2.1 节描述了这些担任参与者的领导者是如何参与下一条规则的实现的。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Commit Wait. 担任协调者的领导者，必须确保客户端不能看到任何被 Ti 提交的数据，直到 TT.after(si)为真。提交等待，就是要确保 si 会比 Ti 的绝对提交时间小。证明如下:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;http://7xnp02.com1.z0.glb.clouddn.com/1752522-fff83c4dbdc512f1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;读写事务&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;正如BigTable一样，Spanner的事务是会将所有的写操作先缓存起来，在Commit的时候一次提交。这样的话，就读不出在同一个事务中写的数据了。不过这没有关系，因为Spanner的数据都是有版本的。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;在读写事务中使用wound-wait算法来避免死锁。当客户端发起一个读写事务的时候，首先是读操作，他先找到相关数据的leader replica，然后加上读锁，读取最近的数据。在客户端事务存活的时候会不断的向leader发心跳，防止超时。当客户端完成了所有的读操作，并且缓存了所有的写操作，就开始了两阶段提交。客户端闲置一个coordinator group，并给每一个leader发送coordinator的id和缓存的写数据。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;leader首先会上一个写锁，他要找一个比现有事务晚的时间戳。通过Paxos记录。每一个相关的都要给coordinator发送他自己准备的那个时间戳。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Coordinatorleader一开始也会上个写锁，当大家发送时间戳给他之后，他就选择一个提交时间戳。这个提交的时间戳，必须比刚刚的所有时间戳晚，而且还要比TT.now()+误差时间 还有晚。这个Coordinator将这个信息记录到Paxos。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;在让replica写入数据生效之前，coordinator还有再等一会。需要等两倍时间误差。这段时间也刚好让Paxos来同步。因为等待之后，在任意机器上发起的下一个事务的开始时间，都比如不会比这个事务的结束时间早了。然后coordinator将提交时间戳发送给客户端还有其他的replica。他们记录日志，写入生效，释放锁。&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;只读事务&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;对于只读事务，Spanner首先要指定一个读事务时间戳。还需要了解在这个读操作中，需要访问的所有的读的Key。Spanner可以自动确定Key的范围。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;如果Key的范围在一个Paxos group内。客户端可以发起一个只读请求给group leader。leader选一个时间戳，这个时间戳要比上一个事务的结束时间要大。然后读取相应的数据。这个事务可以满足外部一致性，读出的结果是最后一次写的结果，并且不会有不一致的数据。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;如果Key的范围在多个Paxos group内，就相对复杂一些。其中一个比较复杂的例子是，可以遍历所有的group leaders，寻找最近的事务发生的时间，并读取。客户端只要时间戳在TT.now().latest之后就可以满足要求了。&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;参考&lt;/h2&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://static.googleusercontent.com/media/research.google.com/zh-CN//archive/spanner-osdi2012.pdf&#34;&gt;Spanner: Google’s Globally-Distributed Database&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://linbingdong.com/2017/02/10/%E5%85%A8%E7%90%83%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%EF%BC%9AGoogle%20Spanner%EF%BC%88%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%EF%BC%89/&#34;&gt;spnner 论文中文版&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://www.voidcn.com/article/p-wxiysjtf-ho.html&#34;&gt;spanner与bigtable&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://blog.jobbole.com/28096/&#34;&gt;Google全球级分布式数据库Spanner原理&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;</content>
    <link href="http://int64.me/2017/Spanner 论文笔记.html"></link>
    <author>
      <name>cwen</name>
    </author>
  </entry>
  <entry>
    <title>Leader Transfer In TiKV</title>
    <updated>2017-10-28T00:00:00Z</updated>
    <id>tag:int64.me,2017-10-28:/2017/Leader Transfer In TiKV.html</id>
    <content type="html">&lt;p&gt;在 TiKV 中，PD 当发现 TiKV 实例上 region 出现 leader 不均匀的时候，会尝试将 leader 从数量比较多的地方 transfer 到其地方，具体调度指令由 PD 发出，TiKV 接收到 PD 的 transfer leader 指令，调用 raft 操作执行真正操作&amp;hellip;&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;先看 PD&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;PD（Placement Driver）在 TiDB 集群里面主要负责 meta 信息存储，以及管理和调度 TiKV 集群， 所有可以想象 Transfer Leader 的命令显然是有 PD 发送到 TiKV。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PD 给 TiKV 发送 Transfer Leader 命令，可以分为一下两类&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;人为干预调度&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;PD 提供 pd-ctl 命令行工具，或是通过 api 接口显示的将一个 region 的 leader 调度到某一个 store 上。&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt; operator add transfer-leader 1 2         // 把 region 1 的 leader 调度到 store 2&#xA;&amp;gt;&amp;gt; operator add transfer-region 1 2 3 4     // 把 region 1 调度到 store 2,3,4&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;pd-ctl 实际上也是请求 PD 的api，具体请求过程略，有兴趣的同学可以去研究一下 PD 的源码。&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;// pd/server/coordinator.go&#xA;func (c *coordinator) sendScheduleCommand(region *core.RegionInfo, step schedule.OperatorStep) {&#xA;        log.Infof(&amp;quot;[region %v] send schedule command: %s&amp;quot;, region.GetId(), step)&#xA;        switch s := step.(type) {&#xA;        case schedule.TransferLeader:&#xA;                cmd := &amp;amp;pdpb.RegionHeartbeatResponse{&#xA;                        TransferLeader: &amp;amp;pdpb.TransferLeader{&#xA;                                Peer: region.GetStorePeer(s.ToStore),&#xA;                        },&#xA;                }&#xA;        .....&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;从上边代码可以看到，其实 PD 的调度命令是通过 heartbeat 来进行传递的，PD 和 TiKV 之间是通过 grpc 通信，当收到到这个操作指令的时候，就会调用 grpc 的send 方法，将请求发送给 TiKV。&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Leader 分布不均匀或是热点过于集中，PD 自身调度&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;PD 的主要作用就是负责管理和调度 TiKV， 如果 TiKV 各个节点上出现了 leader 分布补均匀或是热点 leader 过于集中在某一个 TiKV 节点上的时候，这时候 PD 就会作出干预，进行 transfer leader 等操作。PD 是根据每个 store 或是 leader peer 发送过来的心跳包，来作统计并决定执行哪些操作，每次收到 Region Leader 发来的心跳包时，PD 都会检查是否有对这个 Region 待进行的操作，通过心跳包的回复消息，将需要进行的操作返回给 Region Leader，并在后面的心跳包中监测执行结果。注意这里的操作只是给 Region Leader 的建议，并不保证一定能得到执行，具体是否会执行以及什么时候执行，由 Region Leader 自己根据当前自身状态来定。（后面两句话直接 copy 自 &lt;a href=&#34;https://pingcap.com/blog-tidb-internal-3-zh）。&#34;&gt;https://pingcap.com/blog-tidb-internal-3-zh）。&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;// HandleRegionHeartbeat processes RegionInfo reports from client.&#xA;func (c *RaftCluster) HandleRegionHeartbeat(region *core.RegionInfo) error {&#xA;        if err := c.cachedCluster.handleRegionHeartbeat(region); err != nil {&#xA;                return errors.Trace(err)&#xA;        }&#xA;&#xA;        // If the region peer count is 0, then we should not handle this.&#xA;        if len(region.GetPeers()) == 0 {&#xA;                log.Warnf(&amp;quot;invalid region, zero region peer count - %v&amp;quot;, region)&#xA;                return errors.Errorf(&amp;quot;invalid region, zero region peer count - %v&amp;quot;, region)&#xA;        }&#xA;&#xA;        c.coordinator.dispatch(region)&#xA;        return nil&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;这个函数处理来自 leader peer 的heartbeat， &lt;code&gt;handleRegionHeartbeat&lt;/code&gt; 主要负责更相关region 的信息， 我们主要还是关系 如何发送操作指令，想当然就是 &lt;code&gt;c.coordinator.dispatch(region)&lt;/code&gt; 这个函数干的事情了。&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;func (c *coordinator) dispatch(region *core.RegionInfo) {&#xA;       // Check existed operator.&#xA;       if op := c.getOperator(region.GetId()); op != nil {&#xA;               timeout := op.IsTimeout()&#xA;               if step := op.Check(region); step != nil &amp;amp;&amp;amp; !timeout {&#xA;                       operatorCounter.WithLabelValues(op.Desc(), &amp;quot;check&amp;quot;).Inc()&#xA;                       c.sendScheduleCommand(region, step)&#xA;                       return&#xA;               }&#xA;               if op.IsFinish() {&#xA;                       log.Infof(&amp;quot;[region %v] operator finish: %s&amp;quot;, region.GetId(), op)&#xA;                       operatorCounter.WithLabelValues(op.Desc(), &amp;quot;finish&amp;quot;).Inc()&#xA;                       c.removeOperator(op)&#xA;               } else if timeout {&#xA;                       log.Infof(&amp;quot;[region %v] operator timeout: %s&amp;quot;, region.GetId(), op)&#xA;                       operatorCounter.WithLabelValues(op.Desc(), &amp;quot;timeout&amp;quot;).Inc()&#xA;                       c.removeOperator(op)&#xA;               }&#xA;       }&#xA;    ....&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;可以看到这个函数，先检查是否存在已有的操作，这个 operator 可以有好多种，比如 &lt;code&gt;AddReplica、RemoveReplica、TransferLeader&lt;/code&gt;,&#xA;如果存在检查这个操作显示是到哪一步了，如果是没有结束并且没有超时，就会使用 &lt;code&gt;sendScheduleCommand&lt;/code&gt; 通过 grpc 向这个region 在此发送此次操作。 要是以及完成或是超时分别错处响应的处理并删除这个操作。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;在看 PD 如何将 Transfer leader 这个 opertor 加到 &lt;code&gt;c.operators&lt;/code&gt; 里面的&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;func (l *balanceLeaderScheduler) Schedule(cluster schedule.Cluster) *schedule.Operator {&#xA;        schedulerCounter.WithLabelValues(l.GetName(), &amp;quot;schedule&amp;quot;).Inc()&#xA;        region, newLeader := scheduleTransferLeader(cluster, l.GetName(), l.selector)&#xA;        if region == nil {&#xA;                return nil&#xA;        }&#xA;&#xA;        // Skip hot regions.&#xA;        if cluster.IsRegionHot(region.GetId()) {&#xA;                schedulerCounter.WithLabelValues(l.GetName(), &amp;quot;region_hot&amp;quot;).Inc()&#xA;                return nil&#xA;        }&#xA;&#xA;        source := cluster.GetStore(region.Leader.GetStoreId())&#xA;        target := cluster.GetStore(newLeader.GetStoreId())&#xA;        if !shouldBalance(source, target, core.LeaderKind) {&#xA;                schedulerCounter.WithLabelValues(l.GetName(), &amp;quot;skip&amp;quot;).Inc()&#xA;                return nil&#xA;        }&#xA;        l.limit = adjustBalanceLimit(cluster, core.LeaderKind)&#xA;        schedulerCounter.WithLabelValues(l.GetName(), &amp;quot;new_opeartor&amp;quot;).Inc()&#xA;        step := schedule.TransferLeader{FromStore: region.Leader.GetStoreId(), ToStore: newLeader.GetStoreId()}&#xA;        return schedule.NewOperator(&amp;quot;balance-leader&amp;quot;, region.GetId(), core.LeaderKind, step)&#xA;}&#xA;&#xA;...&#xA;func (c *coordinator) runScheduler(s *scheduleController) {&#xA;        defer c.wg.Done()&#xA;        defer s.Cleanup(c.cluster)&#xA;&#xA;        timer := time.NewTimer(s.GetInterval())&#xA;        defer timer.Stop()&#xA;&#xA;        for {&#xA;                select {&#xA;                case &amp;lt;-timer.C:&#xA;                        timer.Reset(s.GetInterval())&#xA;                        if !s.AllowSchedule() {&#xA;                                continue&#xA;                        }&#xA;                        if op := s.Schedule(c.cluster); op != nil {&#xA;                                c.addOperator(op)&#xA;                        }&#xA;&#xA;                case &amp;lt;-s.Ctx().Done():&#xA;                        log.Infof(&amp;quot;%v stopped: %v&amp;quot;, s.GetName(), s.Ctx().Err())&#xA;                        return&#xA;                }&#xA;        }&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;可以看到&lt;code&gt;Schedule&lt;/code&gt;这个函数最后返回的是一个 &lt;code&gt;operator&lt;/code&gt; , &lt;code&gt;runScheduler&lt;/code&gt; 调用 &lt;code&gt;c.addOperator&lt;/code&gt; 将这个&lt;code&gt;operator&lt;/code&gt; 加到&lt;code&gt;c.operators&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;在看 TiKV&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;接着看 TiKV 从 PD 收到 transfer leader 指令后会做哪些操作。(刚入坑 Rust，看 TiKV 还有点费劲，可能有些地方理解的有问题，还望指出来)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;TiKV 首先是要接受到 PD 发送的命令，来看一个函数，这个函数是用来处理 PD 发送的命令&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;fn schedule_heartbeat_receiver(&amp;amp;mut self, handle: &amp;amp;Handle) {&#xA;        let ch = self.ch.clone();&#xA;        let store_id = self.store_id;&#xA;        let f = self.pd_client&#xA;            .handle_region_heartbeat_response(self.store_id, move |mut resp| {&#xA;                let region_id = resp.get_region_id();&#xA;                let epoch = resp.take_region_epoch();&#xA;                let peer = resp.take_target_peer();&#xA;&#xA;                if resp.has_change_peer() {&#xA;                   // more&#xA;                } else if resp.has_transfer_leader() {&#xA;                    PD_HEARTBEAT_COUNTER_VEC&#xA;                        .with_label_values(&amp;amp;[&amp;quot;transfer leader&amp;quot;])&#xA;                        .inc();&#xA;&#xA;                    let mut transfer_leader = resp.take_transfer_leader();&#xA;                    info!(&#xA;                        &amp;quot;[region {}] try to transfer leader from {:?} to {:?}&amp;quot;,&#xA;                        region_id,&#xA;                        peer,&#xA;                        transfer_leader.get_peer()&#xA;                    );&#xA;                    let req = new_transfer_leader_request(transfer_leader.take_peer());&#xA;                   send_admin_request(&amp;amp;ch, region_id, epoch, peer, req, None)&#xA;                } else {&#xA;                    PD_HEARTBEAT_COUNTER_VEC.with_label_values(&amp;amp;[&amp;quot;noop&amp;quot;]).inc();&#xA;                }&#xA;            })&#xA;    // more&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;这段代码很好理解，就是先从 resp 中读取到 &lt;code&gt;region_id&lt;/code&gt;、&lt;code&gt;peer&lt;/code&gt;，然后在判断要执行的操作是什么，当执行的操作是 &lt;code&gt;transfer_leader&lt;/code&gt;的时候，先是更新一下监控，然后在从 resp 中获取到 &lt;code&gt;leader&lt;/code&gt; 该 &lt;code&gt;transfer&lt;/code&gt; 到什么地方, 在然后呢，就是发送这个命令去执行了。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;先看启动 启动 &lt;code&gt;transfer_leader&lt;/code&gt; 函数&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;pub fn transfer_leader(&amp;amp;mut self, transferee: u64) {&#xA;     let mut m = Message::new();&#xA;     m.set_msg_type(MessageType::MsgTransferLeader);&#xA;     m.set_from(transferee);&#xA;     self.raft.step(m).is_ok();&#xA; }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;这个函数其实很简单，先设置一下消息类型，然后获取到目标 &lt;code&gt;leader&lt;/code&gt;，&lt;code&gt;transferee&lt;/code&gt; 就是目标&lt;code&gt;leader&lt;/code&gt;，当然自己就是当前 &lt;code&gt;leader&lt;/code&gt;。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;在看处理过程&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;fn handle_transfer_leader(&amp;amp;mut self, m: &amp;amp;Message) {&#xA;    let lead_transferee = m.get_from();&#xA;    let last_lead_transferee = self.lead_transferee;&#xA;    if last_lead_transferee.is_some() {&#xA;        if last_lead_transferee.unwrap() == lead_transferee {&#xA;            info!(&#xA;                &amp;quot;{} [term {}] transfer leadership to {} is in progress, ignores request \&#xA;                 to same node {}&amp;quot;,&#xA;                self.tag,&#xA;                self.term,&#xA;                lead_transferee,&#xA;                lead_transferee&#xA;            );&#xA;            return;&#xA;        }&#xA;        self.abort_leader_transfer();&#xA;        info!(&#xA;            &amp;quot;{} [term {}] abort previous transferring leadership to {}&amp;quot;,&#xA;            self.tag,&#xA;            self.term,&#xA;            last_lead_transferee.unwrap()&#xA;        );&#xA;    }&#xA;    if lead_transferee == self.id {&#xA;        debug!(&#xA;            &amp;quot;{} is already leader. Ignored transferring leadership to self&amp;quot;,&#xA;            self.tag&#xA;        );&#xA;        return;&#xA;    }&#xA;    // Transfer leadership to third party.&#xA;    info!(&#xA;        &amp;quot;{} [term {}] starts to transfer leadership to {}&amp;quot;,&#xA;        self.tag,&#xA;        self.term,&#xA;        lead_transferee&#xA;    );&#xA;    // Transfer leadership should be finished in one electionTimeout&#xA;    // so reset r.electionElapsed.&#xA;    self.election_elapsed = 0;&#xA;    self.lead_transferee = Some(lead_transferee);&#xA;    if self.prs[&amp;amp;m.get_from()].matched == self.raft_log.last_index() {&#xA;        self.send_timeout_now(lead_transferee);&#xA;        info!(&#xA;            &amp;quot;{} sends MsgTimeoutNow to {} immediately as {} already has up-to-date log&amp;quot;,&#xA;            self.tag,&#xA;            lead_transferee,&#xA;            lead_transferee&#xA;        );&#xA;    } else {&#xA;        self.send_append(lead_transferee);&#xA;    }&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;这段代码稍有点长，我们可以一步一步的来看， 首先是进行一些检查，第一步是检查是否已经有 &lt;code&gt;transfer leader&lt;/code&gt; 在执行，如果已经正在  &lt;code&gt;transfer leader&lt;/code&gt; 并且目标 &lt;code&gt;leader&lt;/code&gt; 相同的话，就退出这次操作，如果目标不同的话，那就 调用&lt;code&gt;self.abort_leader_transfer();&lt;/code&gt; 这个函数放弃上一次正在执行的 &lt;code&gt;transfer leader&lt;/code&gt;  操作。 紧接就是判断 目标 &lt;code&gt;leader&lt;/code&gt;是不是自己，要是自己那就直接退出就好了，因为不需要&lt;code&gt;transfer leader&lt;/code&gt;。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;下一步就是将目标leader保存在&lt;code&gt;leadTransferee&lt;/code&gt;中，标示着有&lt;code&gt;transfer&lt;/code&gt;正在进行，后续如果有请求&lt;code&gt;propose&lt;/code&gt;进来，会检查这个&lt;code&gt;lead_transferee&lt;/code&gt; 是不是存在，如果存在，其他操作就无法成功，也就是无法进行写操作。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;下一步就是检查 &lt;code&gt;transferee&lt;/code&gt; 和&lt;code&gt;leader&lt;/code&gt;的&lt;code&gt;log&lt;/code&gt;是否一样新&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;如果&lt;code&gt;log&lt;/code&gt; 一致的话就会给&lt;code&gt;transferee&lt;/code&gt;发送&lt;code&gt;MsgTimeoutNow&lt;/code&gt;类型的消息，告诉&lt;code&gt;transferee&lt;/code&gt;可以立即选主，不需要等到&lt;code&gt;election timeout&lt;/code&gt;。&lt;/li&gt;&#xA;&lt;li&gt;如果 &lt;code&gt;log&lt;/code&gt; 不一致，就会给 &lt;code&gt;lead_transferee&lt;/code&gt;  发送一个&lt;code&gt;append&lt;/code&gt; 的请求，追加 &lt;code&gt;log&lt;/code&gt;。 ，leader在收到响应 &lt;code&gt;MsgAppResp&lt;/code&gt;后,如果发现目前正处于&lt;code&gt;transfer leader&lt;/code&gt; 过程中并且 &lt;code&gt;transferee&lt;/code&gt;已经日志最新，则同样，给&lt;code&gt;transferee&lt;/code&gt;发送&lt;code&gt;MsgTimeoutNow&lt;/code&gt;。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;参考&lt;/h2&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://pingcap.com/集群调度/blog-placement-driver-zh&#34;&gt;TiKV 源码解析系列 - Placement Driver&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://pingcap.com/blog-tidb-internal-3-zh&#34;&gt;三篇文章了解 TiDB 技术内幕 - 谈调度&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/27895034&#34;&gt;etcd raft如何实现leadership transfer&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/pingcap/pd&#34;&gt;PD&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/pingcap/tikv&#34;&gt;TiKV&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;</content>
    <link href="http://int64.me/2017/Leader Transfer In TiKV.html"></link>
    <author>
      <name>cwen</name>
    </author>
  </entry>
  <entry>
    <title>分布式系统里的时间、时钟和事件顺序</title>
    <updated>2017-10-22T00:00:00Z</updated>
    <id>tag:int64.me,2017-10-22:/2017/分布式系统里的时间、时钟和事件顺序.html</id>
    <content type="html">&lt;p&gt;以往编写单机程序的时候，如何来判断先后顺序，第一个想到的就是分配一个唯一时间戳，然后根据时间戳的大小来判断事件发生的先后顺序， 但是放在分布式系统就不一定有效&amp;hellip;&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;逻辑时钟和物理时钟&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;物理时间可以用严格的绝对时间来表示事件的发生顺序，但是在分布式的环境中，由于网络等因素无法做到完全的一致的时间，即使使用 NTP 时间，也是会有纳秒的误差。这样就导致在误差事件内发生的事件的先后顺序就很难判断了。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1978 年，Leslie Lamport  在 &lt;a href=&#34;http://amturing.acm.org/p558-lamport.pdf&#34;&gt;Time, Clocks and the Ordering of Events in a Distributed System&lt;/a&gt; 论文中提出了逻辑时钟的概念。在分布式环境中，通过一系列规则来定义逻辑时钟的变化。从而能通过逻辑时钟来对分布式系统中的事件的先后顺序进行判断。逻辑时钟本质上定义了一种 happen before 关系。&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;happen before 关系（偏序关系，部分有序）&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;在分布式系统中，一个进程 process 内的多个事件，自然地具备事件的先后顺序，或者说一个 process 本身就是一个有先验顺序的全序的事件集合。除了 process 内在的顺序，消息的发送和接收事件也是有因果序的。基于这两点，我们定义 happen-before 关系，写作 &lt;code&gt;-&amp;gt;&lt;/code&gt; 。&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;&lt;strong&gt;定义&lt;/strong&gt;: 是一个分布式系统中事件集合上满足如下条件的最小关系：&lt;/h3&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;如果a和b是同一个process内的事件，且a在b之前发生，那么 &lt;code&gt;$$a-&amp;gt;b&lt;/code&gt;。&lt;/li&gt;&#xA;&lt;li&gt;如果a是一个process发送消息事件，b是另一个进程接收该消息的事件，那么&lt;code&gt;$$a-&amp;gt;b&lt;/code&gt;。&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;$$∀a,b,c,a→b,b→c⟹a→c&lt;/code&gt; (即“在……之前发生”的关系具备传递性（transitivity）)&#xA;如果 &lt;code&gt;$$a↛b,b↛a&lt;/code&gt;， 那么称 a 和 b 是并发（concurrent）的。&#xA;根据定义，可以知道happen-before关系 &lt;code&gt;-&amp;gt;&lt;/code&gt; 是一个系统所有事件集合之上的反自反偏序关系。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;h2&gt;逻辑时钟&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;定义&lt;/strong&gt;：定义为一个以事件为输入，输出为某个有序数的函数C。若对所有事件恒满足：&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code class=&#34;language-math&#34;&gt;∀a,b\in E,a→b⟹C(a)&amp;lt;C(b)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;则称该函数C满足时钟约束。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;我们将C函数输出的有序数构成的先后关系，在逻辑上等同于物理时间，我们称由C实现的虚拟时钟为逻辑时钟。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;实现&lt;/strong&gt;: 满足如下两条规则：&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;在同一进程内，C单调递增；&lt;/li&gt;&#xA;&lt;li&gt;如果事件 a 是由进程 Pi发送一个消息 m , 消息 m 包含一个时间戳 T(m) = Ci(a), 进程 Pj 收到一个消息 m，这时候 Pj 设置 Cj 一定大于或等于之的时间，但是一定要比T(m) 大 。&#xA;函数 C 完全满足之前定义的偏序关系。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;h2&gt;全序关系&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;如果&lt;code&gt;$$C(a) = C(b)&lt;/code&gt;，那a、b事件的顺序又是怎样的？假设a、b分别在节点P、Q上发生，Pi、Qj分别表示我们给P、Q的编号，如果 &lt;code&gt;$$C(a) = C(b)&lt;/code&gt; 并且 Pi &amp;lt; Qj，同样定义为a发生在b之前，记作 &lt;code&gt;$$a =&amp;gt; b&lt;/code&gt;。假如我们对下图的A、B、C分别编号&lt;code&gt;$$Ai = 1、Bj = 2、Ck = 3&lt;/code&gt;，因 &lt;code&gt;$$C(B4) = C(C3)&lt;/code&gt; 并且&lt;code&gt;$$Bj &amp;lt; Ck&lt;/code&gt;，则 &lt;code&gt;$$B4 =&amp;gt; C3&lt;/code&gt;。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;通过以上定义，我们可以对所有事件排序、获得事件的全序关系(total order)。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;http://7xnp02.com1.z0.glb.clouddn.com/Screen%20Shot%202017-10-22%20at%205.35.32%20PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;结论&lt;/strong&gt;：→ 关系确定了唯一的部分有序关系；而 ⇒ 则是人为指定的完全排序关系，根据指定规则不同，⇒&lt;/p&gt;&#xA;&#xA;&lt;p&gt;关系可以有无穷多个。（在单一操作系统内的一个最简单的指定规则就是，当两事件时刻相等时，用进程号来分出前后）&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;fault model&lt;/strong&gt;：该结论建立在信息有序到达和信息始终可达的基础上。前者要求一个可靠的传输协议，后者则注定了该模型过于理想化，使得现实的可靠实现不能直接建立在该结论之上。&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;强时钟约束&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;在上述方案中，时钟约束只定义了系统内部的有序性（或称同步性），由于 &lt;code&gt;$$⇒&lt;/code&gt;&#xA;关系的任意性，系统内部的先后关系有可能和系统外部以真实时间为参考系的先后关系相互矛盾。为此，我们必须让系统也能模拟物理的时钟，从而将真实外部世界的先后顺序也考虑在内。处于系统外部的先后关系，我们定义为 (bold arrow, →→)。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;定义&lt;/strong&gt;：若对时钟函数C，满足：&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code class=&#34;language-math&#34;&gt;∀a,b\in E,a→→b⟹C(a)&amp;lt;C(b)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;则称函数满足强时钟约束。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;证明强时钟约束可保证时间上的先后关系略( 数学，有点看不懂，后面在细细研究&#xA;&lt;a href=&#34;http://blog.yangliu.online/2016/09/10/logic-clock-md&#34;&gt;http://blog.yangliu.online/2016/09/10/logic-clock-md&lt;/a&gt; )&#xA;分布式系统理论基础 - 时间、时钟和事件顺序&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;参考&lt;/h2&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://amturing.acm.org/p558-lamport.pdf&#34;&gt;Time, Clocks and the Ordering of Events in a Distributed System&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://blog.yangliu.online/2016/09/10/logic-clock-md/&#34;&gt;分布式系统之逻辑时钟&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://www.cnblogs.com/bangerlee/p/5448766.html&#34;&gt;分布式系统理论基础 - 时间、时钟和事件顺序&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;</content>
    <link href="http://int64.me/2017/分布式系统里的时间、时钟和事件顺序.html"></link>
    <author>
      <name>cwen</name>
    </author>
  </entry>
  <entry>
    <title>rust 笔记 - 构建多线程 web server</title>
    <updated>2017-10-22T00:00:00Z</updated>
    <id>tag:int64.me,2017-10-22:/2017/rust 笔记 - 构建多线程 web server.html</id>
    <content type="html">&lt;p&gt;入坑 rust， 学习如何来用 rust 构建多线程 web server&amp;hellip;&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;先看结构&lt;/h2&gt;&#xA;&#xA;&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;## main.rs&#xA;extern crate hello;&#xA;use hello::ThreadPool;&#xA;use std::io::prelude::*;&#xA;use std::net::TcpListener;&#xA;use std::net::TcpStream;&#xA;use std::fs::File;&#xA;use std::thread;&#xA;use std::time::Duration;&#xA;&#xA;fn main() {&#xA;    let listener = TcpListener::bind(&amp;quot;127.0.0.1:8080&amp;quot;).unwrap();&#xA;&#xA;    let pool = ThreadPool::new(4);&#xA;&#xA;    let mut counter = 0;&#xA;&#xA;    for stream in listener.incoming() {&#xA;        if counter == 2 {&#xA;            println!(&amp;quot;Shutting down.&amp;quot;);&#xA;            break;&#xA;        }&#xA;        counter += 1;&#xA;&#xA;        let stream = stream.unwrap();&#xA;&#xA;        pool.execute(|| {&#xA;            handle_connection(stream);&#xA;        });&#xA;    }&#xA;}&#xA;&#xA;fn handle_connection(mut stream: TcpStream) {&#xA;    let mut buffer = [0;512];&#xA;&#xA;    stream.read(&amp;amp;mut buffer).unwrap();&#xA;&#xA;    let get = b&amp;quot;GET / HTTP/1.1\r\n&amp;quot;;&#xA;    let sleep = b&amp;quot;GET /sleep HTTP/1.1\r\n&amp;quot;;&#xA;&#xA;    let (status_line, filename) = if buffer.starts_with(get) {&#xA;        (&amp;quot;HTTP/1.1 200 OK\r\n\r\n&amp;quot;, &amp;quot;html/hello.html&amp;quot;)&#xA;    } else if buffer.starts_with(sleep) {&#xA;        thread::sleep(Duration::from_secs(5));&#xA;        (&amp;quot;HTTP/1.1 200 OK\r\n\r\n&amp;quot;, &amp;quot;html/hello.html&amp;quot;)&#xA;    } else {&#xA;        (&amp;quot;HTTP/1.1 404 NOT FOUND\r\n\r\n&amp;quot;, &amp;quot;html/404.html&amp;quot;)&#xA;    };&#xA;&#xA;    let mut file = File::open(filename).unwrap();&#xA;    let mut contents = String::new();&#xA;&#xA;    file.read_to_string(&amp;amp;mut contents).unwrap();&#xA;&#xA;    let response = format!(&amp;quot;{}{}&amp;quot;, status_line, contents);&#xA;&#xA;    stream.write(response.as_bytes()).unwrap();&#xA;    stream.flush().unwrap();&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;从代码可以看出，使用 rust 构建一个 web server 并不难。 从头一点点学习&#xA;其他语言一样，要监听 TCP 端口，rust 使用 &lt;code&gt;TcpListener&lt;/code&gt; 中 &lt;code&gt;bind&lt;/code&gt; 函数绑定监听地址，&lt;code&gt;bind&lt;/code&gt; 函数返回 &lt;code&gt;Result&amp;lt;T, E&amp;gt;&lt;/code&gt;, 绑定可能会失败，例如，如果不是管理员尝试连接 80 端口。另一个绑定会失败的情况是两个程序监听相同的端口，这可能发生于运行两个本程序的实例时。&lt;code&gt;unwrap&lt;/code&gt; 取出 &lt;code&gt;T&lt;/code&gt;， 如果出现错误直接 &lt;code&gt;panic&lt;/code&gt;。&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;let pool = ThreadPool::new(4);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;从字母意思可以看出来， 这是创建一个线程池（自己实现的，后面会详细介绍如何实现这个线程池）。&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;for stream in listener.incoming() {&#xA;   let stream = stream.unwrap();&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;TcpListener&lt;/code&gt; 的 &lt;code&gt;incoming&lt;/code&gt; 方法返回一个迭代器，它提供了一系列的流（更准确的说是 &lt;code&gt;TcpStream&lt;/code&gt; 类型的流）。流（&lt;code&gt;stream&lt;/code&gt;）代表一个客户端和服务端之间打开的连接。为此，&lt;code&gt;TcpStream&lt;/code&gt; 允许我们读取它来查看客户端发送了什么，并可以编写响应。所以这个 for 循环会依次处理每个连接并产生一系列的流供我们处理。&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;fn handle_connection(mut stream: TcpStream) {&#xA;    let mut buffer = [0;512];&#xA;    stream.read(&amp;amp;mut buffer).unwrap();&#xA;    let get = b&amp;quot;GET / HTTP/1.1\r\n&amp;quot;;&#xA;    let (status_line, filename) = if buffer.starts_with(get) {&#xA;        (&amp;quot;HTTP/1.1 200 OK\r\n\r\n&amp;quot;, &amp;quot;html/hello.html&amp;quot;)&#xA;    } else {&#xA;        (&amp;quot;HTTP/1.1 404 NOT FOUND\r\n\r\n&amp;quot;, &amp;quot;html/404.html&amp;quot;)&#xA;    };&#xA;    let mut file = File::open(filename).unwrap();&#xA;    let mut contents = String::new();&#xA;    file.read_to_string(&amp;amp;mut contents).unwrap();&#xA;    let response = format!(&amp;quot;{}{}&amp;quot;, status_line, contents);&#xA;    stream.write(response.as_bytes()).unwrap();&#xA;    stream.flush().unwrap();&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;handle_connection&lt;/code&gt; 负责处理请求和响应，在 &lt;code&gt;handle_connection&lt;/code&gt; 中，通过 &lt;code&gt;mut&lt;/code&gt; 关键字将 &lt;code&gt;stream&lt;/code&gt; 参数变为可变。我们将从流中读取数据，所以它需要是可修改的。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;接下来，需要实际读取流。这里分两步进行：首先，在栈上声明一个 &lt;code&gt;buffer&lt;/code&gt; 来存放读取到的数据。这里创建了一个 512 字节的缓冲区，它足以存放基本请求的数据。这对于本章的目的来说是足够的。如果希望处理任意大小的请求，管理所需的缓冲区将更复杂，不过现在一切从简。接着将缓冲区传递给 &lt;code&gt;stream.read&lt;/code&gt; ，它会从 &lt;code&gt;TcpStream&lt;/code&gt;中读取字节并放入缓冲区中。&#xA;接下来就是进行路由判断，当然这里是最简单判断，如果请求是 &amp;ldquo;/&amp;rdquo; 将返回 hello.html 页面否则返回 &amp;ldquo;404.html&amp;rdquo;。&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;实现线程池&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;先看肿么用&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;let pool = ThreadPool::new(4);&#xA;&#xA;    let mut counter = 0;&#xA;&#xA;    for stream in listener.incoming() {&#xA;&#xA;        let stream = stream.unwrap();&#xA;        pool.execute(|| {&#xA;            handle_connection(stream);&#xA;        });&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;首先使用  &lt;code&gt;new&lt;/code&gt; 创建一个大小为 4 的连接池， 并且调用  &lt;code&gt;execute&lt;/code&gt; 方法，&lt;code&gt;execute&lt;/code&gt; 方法的参数可以看出来传入的是一个闭包，并且这个线程只会执行闭包一次。所以判断 execute 参数具有 &lt;code&gt;FnOnce trait bound&lt;/code&gt;, 同时可以从 &lt;code&gt;thread::spawn&lt;/code&gt; 函数的实现推断出。&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; pub fn spawn&amp;lt;F, T&amp;gt;(f: F) -&amp;gt; JoinHandle&amp;lt;T&amp;gt;&#xA;    where&#xA;        F: FnOnce() -&amp;gt; T + Send + &#39;static,&#xA;        T: Send + &#39;static&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;所有我没制动，我的线程池 lib 需有有 &lt;code&gt;ThreadPool&lt;/code&gt; 这个 struct 并且，需要绑定  &lt;code&gt;new&lt;/code&gt; &lt;code&gt;execute&lt;/code&gt; 函数。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;// lib.rs 基本&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;pub struct ThreadPool;&#xA;&#xA;impl ThreadPool {&#xA;    pub fn new(size: u32) -&amp;gt; ThreadPool {&#xA;        ThreadPool&#xA;    }&#xA;    pub fn execute&amp;lt;F&amp;gt;(&amp;amp;self, f: F)&#xA;        where&#xA;            F: FnOnce() + Send + &#39;static&#xA;    {&#xA;&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;F 是这里我们关心的参数；T 与返回值有关所以我们并不关心。考虑到 &lt;code&gt;spawn&lt;/code&gt; 使用 &lt;code&gt;FnOnce&lt;/code&gt; 作为 F 的 &lt;code&gt;trait bound&lt;/code&gt;，这可能也是我们需要的，因为最终会将传递给 &lt;code&gt;execute&lt;/code&gt; 的参数传给 &lt;code&gt;spawn&lt;/code&gt;。因为处理请求的线程只会执行闭包一次，这也进一步确认了 &lt;code&gt;FnOnce&lt;/code&gt; 是我们需要的 &lt;code&gt;trait&lt;/code&gt;。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;F 还有 &lt;code&gt;trait bound Send&lt;/code&gt;和生命周期绑定 &lt;code&gt;&#39;static&lt;/code&gt;，这对我们的情况也是有意义的：需要 Send 来将闭包从一个线程转移到另一个线程，而 &lt;code&gt;&#39;static&lt;/code&gt; 是因为并不知道线程会执行多久。&#xA;&lt;code&gt;FnOnce trait&lt;/code&gt; 仍然需要之后的 ()，因为这里的 &lt;code&gt;FnOnce&lt;/code&gt; 代表一个没有参数也没有返回值的闭包。正如函数的定义，返回值类型可以从签名中省略，不过即便没有参数也需要括号。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;显然这不是一个完整连接池，没法提供服务。 我们接续补全。 结下来我们要做的是要储存它们，显然就是储存事先创建的线程。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;我们定义一个 &lt;code&gt;Worker struct&lt;/code&gt; , &lt;code&gt;ThreadPool&lt;/code&gt; 里面定义一个存放 size 个元素的 vector&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;pub struct ThreadPool {&#xA;    workers: Vec&amp;lt;Worker&amp;gt;,&#xA;}&#xA;&#xA;struct Worker {&#xA;    id: usize,&#xA;    thread: thread::JoinHandle&amp;lt;()&amp;gt;,&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;接下来给 &lt;code&gt;Worker&lt;/code&gt; 添加一个 new 函数&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;impl Worker {&#xA;    fn new(id: usize) -&amp;gt; Worker {&#xA;        let thread = thread::spawn(|| {});&#xA;&#xA;        Worker {&#xA;            id,&#xA;            thread,&#xA;        }&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;在接下来就是考虑如何通信的问题，如何让 ThreodPool 接受到请求然后让worker 去执行，首先想到的就是使用通道了，搞个生产者，消费者了。&#xA;接下来我们要做什么&#xA;    1. ThreadPool 会创建一个通道并充当发送端。&#xA;    2. 每个 Worker 将会充当通道的接收端。&#xA;    3. 新建一个 Job 结构体来存放用于向通道中发送的闭包。&#xA;    4. ThreadPool 的 execute 方法会在发送端发出期望执行的任务。&#xA;    5. 在线程中，Worker 会遍历通道的接收端并执行任何接收到的任务。&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;// ...snip...&#xA;use std::sync::mpsc;&#xA;&#xA;pub struct ThreadPool {&#xA;    workers: Vec&amp;lt;Worker&amp;gt;,&#xA;    sender: mpsc::Sender&amp;lt;Job&amp;gt;,&#xA;}&#xA;&#xA;struct Job;&#xA;&#xA;impl ThreadPool {&#xA;    // ...snip...&#xA;    pub fn new(size: usize) -&amp;gt; ThreadPool {&#xA;        assert!(size &amp;gt; 0);&#xA;&#xA;        let (sender, receiver) = mpsc::channel();&#xA;&#xA;        let mut workers = Vec::with_capacity(size);&#xA;&#xA;        for id in 0..size {&#xA;            workers.push(Worker::new(id));&#xA;        }&#xA;&#xA;        ThreadPool {&#xA;            workers,&#xA;            sender,&#xA;        }&#xA;    }&#xA;    // ...snip...&#xA;}&#xA;&#xA;impl Worker {&#xA;    fn new(id: usize, receiver: mpsc::Receiver&amp;lt;Job&amp;gt;) -&amp;gt; Worker {&#xA;        let thread = thread::spawn(|| {&#xA;            receiver;&#xA;        });&#xA;&#xA;        Worker {&#xA;            id,&#xA;            thread,&#xA;        }&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;ThreadPool&lt;/code&gt; 来储存一个发送 Job 实例的通道发送端&lt;/p&gt;&#xA;&#xA;&lt;p&gt;在 &lt;code&gt;ThreadPool::new&lt;/code&gt; 中，新建了一个通道，并接着让线程池在接收端等待。这段代码能够编译，不过仍有警告。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;在线程池创建每个 worker 时将通道的接收端传递给他们。须知我们希望在 &lt;code&gt;worker&lt;/code&gt; 所分配的线程中使用通道的接收端，所以将在闭包中引用 &lt;code&gt;receiver&lt;/code&gt; 参数。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;显然上边的消费者是有问题。 Rust 所提供的通道实现是多生产者，单消费者的，所以不能简单的克隆通道的消费端来解决问题。即便可以我们也不希望克隆消费端；在所有的&lt;code&gt;worker&lt;/code&gt; 中共享单一 &lt;code&gt;receiver&lt;/code&gt; 才是我们希望的在线程间分发任务的机制。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;另外，从通道队列中取出任务涉及到修改 &lt;code&gt;receiver&lt;/code&gt;，所以这些线程需要一个能安全的共享和修改 &lt;code&gt;receiver&lt;/code&gt; 的方式。如果修改不是线程安全的，则可能遇到竞争状态，例如两个线程因同时在队列中取出相同的任务并执行了相同的工作。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;我们可以使用线程安全智能指针，为了在多个线程间共享所有权并允许线程修改其值，需要使用 &lt;code&gt;Arc&amp;lt;Mutex&amp;lt;T&amp;gt;&amp;gt;&lt;/code&gt;。&lt;code&gt;Arc&lt;/code&gt; 使得多个 &lt;code&gt;worker&lt;/code&gt; 拥有接收端，而 &lt;code&gt;Mutex&lt;/code&gt; 则确保一次只有一个 &lt;code&gt;worker&lt;/code&gt; 能从接收端得到任务。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;so 我的代码就改成这样了&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; use std::sync::Arc;&#xA;use std::sync::Mutex;&#xA;&#xA;// ...snip...&#xA;&#xA;impl ThreadPool {&#xA;    // ...snip...&#xA;    pub fn new(size: usize) -&amp;gt; ThreadPool {&#xA;        assert!(size &amp;gt; 0);&#xA;&#xA;        let (sender, receiver) = mpsc::channel();&#xA;&#xA;        let receiver = Arc::new(Mutex::new(receiver));&#xA;&#xA;        let mut workers = Vec::with_capacity(size);&#xA;&#xA;        for id in 0..size {&#xA;            workers.push(Worker::new(id, receiver.clone()));&#xA;        }&#xA;&#xA;        ThreadPool {&#xA;            workers,&#xA;            sender,&#xA;        }&#xA;    }&#xA;    // ...snip...&#xA;}&#xA;impl Worker {&#xA;    fn new(id: usize, receiver: Arc&amp;lt;Mutex&amp;lt;mpsc::Receiver&amp;lt;Job&amp;gt;&amp;gt;&amp;gt;) -&amp;gt; Worker {&#xA;        // ...snip...&#xA;    }&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;让我们实现 &lt;code&gt;ThreadPool&lt;/code&gt; 上的 &lt;code&gt;execute&lt;/code&gt; 方法。同时也要修改 &lt;code&gt;Job&lt;/code&gt; 结构体：它将不再是结构体，&lt;code&gt;Job&lt;/code&gt; 将是一个有着 &lt;code&gt;execute&lt;/code&gt; 接收到的闭包类型的 &lt;code&gt;trait&lt;/code&gt; 对象的类型别名。在 worker 中，传递给 &lt;code&gt;thread::spawn&lt;/code&gt; 的闭包仍然还只是引用了通道的接收端。但是我们需要闭包一直循环，向通道的接收端请求任务，并在得到任务时执行他们。&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&#xA;type Job = Box&amp;lt;FnOnce() + Send + &#39;static&amp;gt;;&#xA;&#xA;impl ThreadPool {&#xA;    // ...snip...&#xA;&#xA;    pub fn execute&amp;lt;F&amp;gt;(&amp;amp;self, f: F)&#xA;        where&#xA;            F: FnOnce() + Send + &#39;static&#xA;    {&#xA;        let job = Box::new(f);&#xA;&#xA;        self.sender.send(job).unwrap();&#xA;    }&#xA;}&#xA;&#xA;&#xA;impl Worker {&#xA;    fn new(id: usize, receiver: Arc&amp;lt;Mutex&amp;lt;mpsc::Receiver&amp;lt;Job&amp;gt;&amp;gt;&amp;gt;) -&amp;gt; Worker {&#xA;        let thread = thread::spawn(move || {&#xA;            loop {&#xA;                let job = receiver.lock().unwrap().recv().unwrap();&#xA;&#xA;                println!(&amp;quot;Worker {} got a job; executing.&amp;quot;, id);&#xA;&#xA;                (*job)();&#xA;            }&#xA;        });&#xA;&#xA;        Worker {&#xA;            id,&#xA;            thread,&#xA;        }&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;如果现在编译我们的代码，还是会出现错误&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;error[E0161]: cannot move a value of type std::ops::FnOnce() +&#xA;std::marker::Send: the size of std::ops::FnOnce() + std::marker::Send cannot be&#xA;statically determined&#xA;  --&amp;gt; src/lib.rs:63:17&#xA;   |&#xA;63 |                 (*job)();&#xA;   |                 ^^^^^^&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;为了调用储存在 &lt;code&gt;Box&amp;lt;T&amp;gt;&lt;/code&gt;（这正是 Job 别名的类型）中的 &lt;code&gt;FnOnce&lt;/code&gt; 闭包，该闭包需要能将自己移动出 &lt;code&gt;Box&amp;lt;T&amp;gt;&lt;/code&gt;，因为当调用这个闭包时，它获取 &lt;code&gt;self&lt;/code&gt; 的所有权。通常来说，将值移动出 &lt;code&gt;Box&amp;lt;T&amp;gt;&lt;/code&gt; 是不被允许的，因为 Rust 不知道 &lt;code&gt;Box&amp;lt;T&amp;gt;&lt;/code&gt; 中的值将会有多大。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;我们可以使用 &lt;code&gt;self: Box&amp;lt;Self&amp;gt;&lt;/code&gt;语法的方法，获取了储存在 &lt;code&gt;Box&amp;lt;T&amp;gt;&lt;/code&gt; 中的 &lt;code&gt;Self&lt;/code&gt; 值的所有权。这正是我们希望做的，然而不幸的是 Rust 调用闭包的那部分实现并没有使用 &lt;code&gt;self: Box&amp;lt;Self&amp;gt;&lt;/code&gt;。所以这里 Rust 也不知道它可以使用 &lt;code&gt;self: Box&amp;lt;Self&amp;gt;&lt;/code&gt; 来获取闭包的所有权并将闭包移动出 &lt;code&gt;Box&amp;lt;T&amp;gt;&lt;/code&gt;。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;不过目前让我们绕过这个问题。所幸有一个技巧可以显式的告诉 Rust 我们处于可以获取使用 &lt;code&gt;self: Box&amp;lt;Self&amp;gt;&lt;/code&gt; 的 &lt;code&gt;Box&amp;lt;T&amp;gt;&lt;/code&gt; 中值的所有权的状态，而一旦获取了闭包的所有权就可以调用它了。这涉及到定义一个新 &lt;code&gt;trait&lt;/code&gt;，它带有一个在签名中使用 &lt;code&gt;self: Box&amp;lt;Self&amp;gt;&lt;/code&gt; 的方法 &lt;code&gt;call_box&lt;/code&gt;，为任何实现了 &lt;code&gt;FnOnce()&lt;/code&gt;的类型定义这个 &lt;code&gt;trait&lt;/code&gt;，修改类型别名来使用这个新 &lt;code&gt;trait&lt;/code&gt;，并修改 &lt;code&gt;Worker&lt;/code&gt; 使用 &lt;code&gt;call_box&lt;/code&gt; 方法&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;trait FnBox {&#xA;    fn call_box(self: Box&amp;lt;Self&amp;gt;);&#xA;}&#xA;&#xA;impl&amp;lt;F: FnOnce()&amp;gt; FnBox for F {&#xA;    fn call_box(self: Box&amp;lt;F&amp;gt;) {&#xA;        (*self)()&#xA;    }&#xA;}&#xA;&#xA;type Job = Box&amp;lt;FnBox + Send + &#39;static&amp;gt;;&#xA;&#xA;// ...snip...&#xA;&#xA;impl Worker {&#xA;    fn new(id: usize, receiver: Arc&amp;lt;Mutex&amp;lt;mpsc::Receiver&amp;lt;Job&amp;gt;&amp;gt;&amp;gt;) -&amp;gt; Worker {&#xA;        let thread = thread::spawn(move || {&#xA;            loop {&#xA;                let job = receiver.lock().unwrap().recv().unwrap();&#xA;&#xA;                println!(&amp;quot;Worker {} got a job; executing.&amp;quot;, id);&#xA;&#xA;                job.call_box();&#xA;            }&#xA;        });&#xA;&#xA;        Worker {&#xA;            id,&#xA;            thread,&#xA;        }&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;Graceful Shutdown 与清理&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;如果我们编译上述表示的代码，会有一些警告说存在一些字段并没有直接被使用，这提醒了我们并没有清理任何内容。当使用 &lt;code&gt;ctrl-C&lt;/code&gt; 终止主线程，所有其他线程也会立刻停止，即便他们正在处理一个请求。现在我们要为 &lt;code&gt;ThreadPool&lt;/code&gt; 实现 &lt;code&gt;Drop trait&lt;/code&gt; 对线程池中的每一个线程调用 &lt;code&gt;join&lt;/code&gt;，这样这些线程将会执行完他们的请求。接着会为 &lt;code&gt;ThreadPool&lt;/code&gt; 实现一个方法来告诉线程他们应该停止接收新请求并结束。为了实践这些代码，修改 &lt;code&gt;server&lt;/code&gt; 在 &lt;code&gt;graceful Shutdown&lt;/code&gt; 之前只接受两个请求。&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;enum Message {&#xA;    NewJob(Job),&#xA;    Terminate,&#xA;}&#xA;&#xA;pub struct ThreadPool {&#xA;    workers: Vec&amp;lt;Worker&amp;gt;,&#xA;    sender: mpsc::Sender&amp;lt;Message&amp;gt;,&#xA;}&#xA;&#xA;// ...snip...&#xA;&#xA;impl ThreadPool {&#xA;    // ...snip...&#xA;    pub fn new(size: usize) -&amp;gt; ThreadPool {&#xA;        assert!(size &amp;gt; 0);&#xA;&#xA;        let (sender, receiver) = mpsc::channel();&#xA;&#xA;        // ...snip...&#xA;    }&#xA;&#xA;    pub fn execute&amp;lt;F&amp;gt;(&amp;amp;self, f: F)&#xA;        where&#xA;            F: FnOnce() + Send + &#39;static&#xA;    {&#xA;        let job = Box::new(f);&#xA;&#xA;        self.sender.send(Message::NewJob(job)).unwrap();&#xA;    }&#xA;}&#xA;&#xA;// ...snip...&#xA;&#xA;impl Worker {&#xA;    fn new(id: usize, receiver: Arc&amp;lt;Mutex&amp;lt;mpsc::Receiver&amp;lt;Message&amp;gt;&amp;gt;&amp;gt;) -&amp;gt;&#xA;        Worker {&#xA;&#xA;        let thread = thread::spawn(move ||{&#xA;            loop {&#xA;                let message = receiver.lock().unwrap().recv().unwrap();&#xA;&#xA;                match message {&#xA;                    Message::NewJob(job) =&amp;gt; {&#xA;                        println!(&amp;quot;Worker {} got a job; executing.&amp;quot;, id);&#xA;&#xA;                        job.call_box();&#xA;                    },&#xA;                    Message::Terminate =&amp;gt; {&#xA;                        println!(&amp;quot;Worker {} was told to terminate.&amp;quot;, id);&#xA;&#xA;                        break;&#xA;                    },&#xA;                }&#xA;            }&#xA;        });&#xA;&#xA;        Worker {&#xA;            id,&#xA;            thread: Some(thread),&#xA;        }&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;收发 &lt;code&gt;Message&lt;/code&gt; 值并在 Worker 收到 &lt;code&gt;Message::Terminate&lt;/code&gt; 时退出循环&lt;/p&gt;&#xA;&#xA;&lt;p&gt;需要将 &lt;code&gt;ThreadPool&lt;/code&gt; 定义、创建通道的 &lt;code&gt;ThreadPool::new&lt;/code&gt; 和 &lt;code&gt;Worker::new&lt;/code&gt; 签名中的 &lt;code&gt;Job&lt;/code&gt; 改为 &lt;code&gt;Message&lt;/code&gt;。&lt;code&gt;ThreadPool&lt;/code&gt; 的 &lt;code&gt;execute&lt;/code&gt; 方法需要发送封装进 &lt;code&gt;Message::NewJob&lt;/code&gt; 成员的任务，当获取到 &lt;code&gt;NewJob&lt;/code&gt; 时会处理任务而收到 &lt;code&gt;Terminate&lt;/code&gt; 成员时则会退出循环。&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;impl Drop for ThreadPool {&#xA;    fn drop(&amp;amp;mut self) {&#xA;        println!(&amp;quot;Sending terminate message to all workers.&amp;quot;);&#xA;&#xA;        for _ in &amp;amp;mut self.workers {&#xA;            self.sender.send(Message::Terminate).unwrap();&#xA;        }&#xA;&#xA;        println!(&amp;quot;Shutting down all workers.&amp;quot;);&#xA;&#xA;        for worker in &amp;amp;mut self.workers {&#xA;            println!(&amp;quot;Shutting down worker {}&amp;quot;, worker.id);&#xA;&#xA;            if let Some(thread) = worker.thread.take() {&#xA;                thread.join().unwrap();&#xA;            }&#xA;        }&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;在对每个 &lt;code&gt;worker&lt;/code&gt; 线程调用 &lt;code&gt;join&lt;/code&gt; 之前向 &lt;code&gt;worker&lt;/code&gt; 发送 &lt;code&gt;Message::Terminate&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;现在遍历了 &lt;code&gt;worker&lt;/code&gt; 两次，一次向每个 &lt;code&gt;worker&lt;/code&gt; 发送一个 &lt;code&gt;Terminate&lt;/code&gt; 消息，一个调用每个 &lt;code&gt;worker&lt;/code&gt; 线程上的 &lt;code&gt;join&lt;/code&gt;。如果尝试在同一循环中发送消息并立即 &lt;code&gt;join&lt;/code&gt; 线程，则无法保证当前迭代的 &lt;code&gt;worker&lt;/code&gt; 是从通道收到终止消息的&lt;code&gt;worker&lt;/code&gt;。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;为了更好的理解为什么需要两个分开的循环，想象一下只有两个 &lt;code&gt;worker&lt;/code&gt; 的场景。如果在一个循环中遍历每个&lt;code&gt;worker&lt;/code&gt;，在第一次迭代中 &lt;code&gt;worker&lt;/code&gt; 是第一个 &lt;code&gt;worker&lt;/code&gt;，我们向通道发出终止消息并对第一个 &lt;code&gt;worker&lt;/code&gt; 线程调用 &lt;code&gt;join&lt;/code&gt;。如果第一个 &lt;code&gt;worker&lt;/code&gt; 当时正忙于处理请求，则第二个 &lt;code&gt;worker&lt;/code&gt; 会从通道接收这个终止消息并结束。而我们在等待第一个 &lt;code&gt;worker&lt;/code&gt; 结束，不过它永远也不会结束因为第二个线程取走了终止消息。现在我们就阻塞在了等待第一个 &lt;code&gt;worker&lt;/code&gt; 结束，而无法发出第二条终止消息。死锁！&lt;/p&gt;&#xA;&#xA;&lt;p&gt;为了避免此情况，首先从通道中取出所有的 &lt;code&gt;Terminate&lt;/code&gt; 消息，接着 &lt;code&gt;join&lt;/code&gt; 所有的线程。因为每个 &lt;code&gt;worker&lt;/code&gt; 一旦收到终止消息即会停止从通道接收消息，我们就可以确保如果发送同 &lt;code&gt;worker&lt;/code&gt; 数相同的终止消息，在 &lt;code&gt;join&lt;/code&gt; 之前每个线程都会收到一个终止消息。&lt;/p&gt;&#xA;&#xA;&lt;h4&gt;简易线程池完整代码&lt;/h4&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;// lib.rs&#xA;use std::thread;&#xA;use std::sync::mpsc;&#xA;use std::sync::Arc;&#xA;use std::sync::Mutex;&#xA;&#xA;enum Message {&#xA;    NewJob(Job),&#xA;    Terminate,&#xA;}&#xA;&#xA;pub struct ThreadPool{&#xA;    workers: Vec&amp;lt;Worker&amp;gt;,&#xA;    sender: mpsc::Sender&amp;lt;Message&amp;gt;,&#xA;}&#xA;&#xA;type Job = Box&amp;lt;FnBox + Send + &#39;static&amp;gt;;&#xA;&#xA;impl ThreadPool {&#xA;    /// Create a new ThreadPool.&#xA;    ///&#xA;    /// The size is the number of threads in the pool.&#xA;    ///&#xA;    /// # Panics&#xA;    ///&#xA;    /// The `new` function will panic if the size is zero.&#xA;    pub fn new(size :usize) -&amp;gt;ThreadPool{&#xA;        assert!(size &amp;gt; 0);&#xA;&#xA;        let (sender, receiver) = mpsc::channel();&#xA;&#xA;        let receiver = Arc::new(Mutex::new(receiver));&#xA;&#xA;        let mut workers = Vec::with_capacity(size);&#xA;&#xA;        for id in 0..size {&#xA;            workers.push(Worker::new(id, receiver.clone()));&#xA;        }&#xA;&#xA;        ThreadPool {&#xA;            workers,&#xA;            sender,&#xA;        }&#xA;    }&#xA;&#xA;    pub fn execute&amp;lt;F&amp;gt; (&amp;amp;self, f: F)&#xA;        where&#xA;        F: FnOnce() + Send + &#39;static&#xA;    {&#xA;        let job = Box::new(f);&#xA;        self.sender.send(Message::NewJob(job)).unwrap();&#xA;    }&#xA;}&#xA;&#xA;impl Drop for ThreadPool {&#xA;    fn drop(&amp;amp;mut self) {&#xA;        println!(&amp;quot;Sending terminate message to all workers.&amp;quot;);&#xA;&#xA;        for _ in &amp;amp;mut self.workers {&#xA;            self.sender.send(Message::Terminate).unwrap();&#xA;        }&#xA;&#xA;        println!(&amp;quot;Shutting down all workers.&amp;quot;);&#xA;&#xA;        for worker in &amp;amp;mut self.workers {&#xA;            println!(&amp;quot;Shutting down worker {}&amp;quot;, worker.id);&#xA;&#xA;            if let Some(thread) = worker.thread.take() {&#xA;                thread.join().unwrap();&#xA;            }&#xA;        }&#xA;    }&#xA;}&#xA;&#xA;trait FnBox {&#xA;    fn call_box(self: Box&amp;lt;Self&amp;gt;);&#xA;}&#xA;&#xA;impl&amp;lt;F: FnOnce()&amp;gt; FnBox for F {&#xA;    fn call_box(self: Box&amp;lt;F&amp;gt;) {&#xA;        (*self)()&#xA;    }&#xA;}&#xA;&#xA;struct Worker {&#xA;    id: usize,&#xA;    thread: Option&amp;lt;thread::JoinHandle&amp;lt;()&amp;gt;&amp;gt;,&#xA;}&#xA;&#xA;impl Worker {&#xA;    fn new(id: usize, receiver: Arc&amp;lt;Mutex&amp;lt;mpsc::Receiver&amp;lt;Message&amp;gt;&amp;gt;&amp;gt;) -&amp;gt; Worker {&#xA;        let thread = thread::spawn(move || {&#xA;            loop {&#xA;                let message = receiver.lock().unwrap().recv().unwrap();&#xA;&#xA;                match message {&#xA;                    Message::NewJob(job) =&amp;gt; {&#xA;                        println!(&amp;quot;Worker {} got a job; executing.&amp;quot;, id);&#xA;&#xA;                        job.call_box();&#xA;                    },&#xA;                    Message::Terminate =&amp;gt; {&#xA;                        println!(&amp;quot;Worker {} was told to terminate.&amp;quot;, id);&#xA;&#xA;                        break;&#xA;                    },&#xA;                }&#xA;            }&#xA;        });&#xA;&#xA;        Worker {&#xA;            id,&#xA;            thread: Some(thread),&#xA;        }&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;参考&lt;/h2&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://doc.rust-lang.org/book/second-edition/ch20-00-final-project-a-web-server.html&#34;&gt;The Rust Programming Language&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kaisery.github.io/trpl-zh-cn/&#34;&gt;Rust程序设计-中文版&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;</content>
    <link href="http://int64.me/2017/rust 笔记 - 构建多线程 web server.html"></link>
    <author>
      <name>cwen</name>
    </author>
  </entry>
  <entry>
    <title>GFS 笔记 - MIT-6.824</title>
    <updated>2017-08-31T00:00:00Z</updated>
    <id>tag:int64.me,2017-08-31:/2017/GFS 笔记 - MIT-6.824.html</id>
    <content type="html">&lt;p&gt;GFS是一个可扩展的分布式文件系统，用于大型的、分布式的、对大量数据进行访问的应用。它运行于廉价的普通硬件上，并提供容错功能。它可以给大量的用户提供总体性能较高的服务&amp;hellip;&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;什么是一致性？&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;一个正确性条件&lt;/li&gt;&#xA;&lt;li&gt;当存在副本和程序并发访问的时候，一致性是很重要的&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;如果一个应用进行写操作，那么之后的读操作可以观察到什么？如果这个读操作来自其他应用程序又会看到什么？&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;弱一致性&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;read() 可能返回旧的数据 &amp;mdash; 不是最新写入的数据&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;强一致性&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;read() 总是返回最新写入的数据&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;一般的权衡&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;强一致性对程序的写操作（application writers）表现不错&lt;/li&gt;&#xA;&lt;li&gt;强一致性对性能有一定的影响&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;更多的正确性条件（通常被称为一致性模型）&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;&amp;ldquo;理想&amp;rdquo; 的一致性模型&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;一个有副本的文件系统和一个没有副本的文件系统表现的一样，就像许多客户端访问痛一台机器上的单独磁盘&lt;/li&gt;&#xA;&lt;li&gt;如果一个程序写数据，那么之后的就可以读到之前写的数据&lt;/li&gt;&#xA;&lt;li&gt;如果两个程序并发的写同一个文件呢？&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;在文件系统中这种行为经常是未定义的 —— 文件也许会混合两个写操作的内容&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;如果两个程序并发的写同一个目录呢？&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;一个一个顺序执行&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;去实现理想的一致性的挑战&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;并发&lt;/li&gt;&#xA;&lt;li&gt;机器失败&lt;/li&gt;&#xA;&lt;li&gt;网络分割&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;为什么去克服这些挑战是困难的？&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;clients 和 servers 之前要求通信，可能会消耗性能&lt;/li&gt;&#xA;&lt;li&gt;协议可能变得复杂 &amp;mdash;  后面的课程我们会看到很难实现正确的系统&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;GFS 中的主要挑战&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;存在许多机器，所有机器出现故障是很常见的现象，假设一台机器一年出错一次，那么当存在1000台机器的时候，每天都有三台机器出现问题。&lt;/li&gt;&#xA;&lt;li&gt;高性能：很多并发的读写操作，Map/Reduce工作会从 GFS 读取数据，然后保存最后的结果，注意：保存的不是中间临时文件。&lt;/li&gt;&#xA;&lt;li&gt;有效的使用网络&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;高层次的设计&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;定义目录、文件、命名、打开/读/写操作，但是不是符合posix标准&lt;/li&gt;&#xA;&lt;li&gt;成千上百的带有硬盘的 linux 服务器&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;存储64MB的块(an ordinary Linux file for each chunk)&lt;/li&gt;&#xA;&lt;li&gt;each chunk 在三个服务器上存在副本&lt;/li&gt;&#xA;&lt;li&gt;Q: 为什么三副本？&lt;/li&gt;&#xA;&lt;li&gt;Q：除了数据的可用性，三副本方案给我们带来了什么？对热点文件的读取做负载均衡&lt;/li&gt;&#xA;&lt;li&gt;Q: 为什么不只存储一份文件到 RAID 磁盘？ RAID 磁盘不是常用品，我们想给整台机器做容错，而不是仅仅针对存储系统。&lt;/li&gt;&#xA;&lt;li&gt;Q: 为什么 chunks 这么大？&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;GFS 的 master server 知道目录层级&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;对于目录而言，知道里面有哪些文件&lt;/li&gt;&#xA;&lt;li&gt;对于为难而言，知道哪些数据块服务器存储了相关的64MB大小数据块&lt;/li&gt;&#xA;&lt;li&gt;master server 在内存里面保存状态信息，每个chunk在主控服务器上面只保存64bytes大小的metadata&lt;/li&gt;&#xA;&lt;li&gt;master server 有为元数据准备的可回收数据库，可以从断电故障后快速恢复&lt;/li&gt;&#xA;&lt;li&gt;同时存在备份的主控服务器(shadow master)，数据略比主控服务器服务器延迟，可以被提升为主控服务器&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;基础的文件操作&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;客户端读操作&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;客户端发送文件名和偏移量到 master&lt;/li&gt;&#xA;&lt;li&gt;主控服务器回复带有相关 chunk 的数据块服务器集合，相应信息包括版本信息，客户端临时缓存这些信息，然后访问最近的数据块服务器，版本检查，如果版本是不正确的，重新从 master 获取最新的数据&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;客户端 append&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;客户端询问 master 去存储在什么地方&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;如果文件大小超过64MB，master 也许会选择一些新的数据块服务器&lt;/li&gt;&#xA;&lt;li&gt;master 把 chunk servers 和版本信息返回给客户端，其中一个 chunk 是 primary&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;客户端把数据推送的副本&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;副本构成一个链表&lt;/li&gt;&#xA;&lt;li&gt;链表涉及到网络拓扑&lt;/li&gt;&#xA;&lt;li&gt;允许快速复杂&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;当数据都在所有的 chunk servers 时候客户端与 primary server 沟通&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;primary 分配序列号&lt;/li&gt;&#xA;&lt;li&gt;primary apply 本地的修改&lt;/li&gt;&#xA;&lt;li&gt;primary 转发请求到副本&lt;/li&gt;&#xA;&lt;li&gt;当 primary 收到所有副本的 ack 后，返回给客户端&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;如果一个副本是没有响应，客户端将要重试&lt;/li&gt;&#xA;&lt;li&gt;如果 master 没有重置 lease， master 可以重新指定新的 master，&lt;/li&gt;&#xA;&lt;li&gt;如果副本的数量小于某个值的时候，master 会重新添加副本，重新负载副本&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;GFS 达到了理想中的一致性了吗？&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;两种情况：目录和文件&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;目录 yes，but&amp;hellip;&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;yes: 强一致性 （仅有一个副本)&lt;/li&gt;&#xA;&lt;li&gt;but:&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Master 有可能挂掉并且 GFS 是不可用的&lt;/li&gt;&#xA;&lt;li&gt;shadow master 可以提供只读操作，但是它返回老的数据&lt;/li&gt;&#xA;&lt;li&gt;Q: 那写操作呢？ 脑裂现象&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h3&gt;文件：不一定&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;带有原子追加的突变&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;一个文件可以有重复的 enties 和空洞，如果 primary 与一个副本连接失败，primary 会给客户端报一个错误，客户端重试并且 primary 选择一个新的偏移量，记录可能是重复写入，其他的副本可能存在空洞&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;一个不幸运的客户端可能在短时间内读到一个老的数据&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;一个失败的突变导致 chunks 的不一致，primary chunk 更新 chunck 但是失败了并且副本数据过时了&lt;/li&gt;&#xA;&lt;li&gt;一个客户端可能去读一个没有跟新数据的 chunk&lt;/li&gt;&#xA;&lt;li&gt;当客户端刷新 lease  的时候，他将获得到最新版本&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;没有原子追加的突变&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;不同客户端的数据可能被混合&lt;/li&gt;&#xA;&lt;li&gt;如果你是在使用原子追加或是临时文件和自动重命名，并发的写在不同的 Unix 机器上可能导致奇怪的现象&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;作者主张弱一致性对app而言不是什么大问题&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;大多数文件更新操作只是追加&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;应用程序可以使用添加记录中的uid判断是否重复&lt;/li&gt;&#xA;&lt;li&gt;应用程序也许只是读取到少量的数据（而不是不新鲜的数据）&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;应用程序可以使用临时文件和原子的重命名操作&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;性能&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;巨大的读操作总吞吐量（3个副本，striping ？？？）&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;125 MB/sec&lt;/li&gt;&#xA;&lt;li&gt;接近网络饱和状态&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;写入不同的文件低于可能的最大值&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;作者怪网络堆栈&lt;/li&gt;&#xA;&lt;li&gt;chunk直接的复制操作会引起延迟&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;并发追加同一份文件&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;被服务器存在的最新的chunk所限制&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;总结&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;GFS使用的比较重要的容错技术riz&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;操作日志、检查点&lt;/li&gt;&#xA;&lt;li&gt;chunk之间的主备备份（but with consistencies？？）&lt;/li&gt;&#xA;&lt;li&gt;我们将会在其他系统中也看到这里&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;哪些在GFS中工作很好&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;巨大的顺序读写操作&lt;/li&gt;&#xA;&lt;li&gt;追加&lt;/li&gt;&#xA;&lt;li&gt;巨大的吞吐量&lt;/li&gt;&#xA;&lt;li&gt;数据之间的容错&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;哪些在GFS中做的不怎么好&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;master服务器的容错&lt;/li&gt;&#xA;&lt;li&gt;小文件（master服务器的瓶颈）&lt;/li&gt;&#xA;&lt;li&gt;多个客户端并发的向同一份文件更新操作（除了追加）&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;参考&lt;/h2&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://pdos.csail.mit.edu/6.824/papers/gfs.pdf&#34;&gt;The Google File System&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://pdos.csail.mit.edu/6.824/notes/l-gfs-short.txt&#34;&gt;6.824-nodes&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/feixiao/Distributed-Systems/blob/master/Lec03_GFS/GFS.md&#34;&gt;GFS案例学习&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;</content>
    <link href="http://int64.me/2017/GFS 笔记 - MIT-6.824.html"></link>
    <author>
      <name>cwen</name>
    </author>
  </entry>
  <entry>
    <title>RPC in GO - MIT-6.824</title>
    <updated>2017-08-10T00:00:00Z</updated>
    <id>tag:int64.me,2017-08-10:/2017/RPC in GO - MIT-6.824.html</id>
    <content type="html">&lt;p&gt;RPC 理想上想把网络通信做的跟函数调用一样&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;远程调用 (RPC)&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;分布式系统的关键模块&lt;/li&gt;&#xA;&lt;li&gt;目的：&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;容易编写网络通信程序&lt;/li&gt;&#xA;&lt;li&gt;隐藏大多数 client/server 之间通信的细节&lt;/li&gt;&#xA;&lt;li&gt;客户端调用更加像传统的过程调用&lt;/li&gt;&#xA;&lt;li&gt;服务端处理更加像传统的过程调用&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;RPC 已经被广泛应用&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;RPC 理想上想把网络通信就当做函数调用一样简单&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Client:&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    z = fn(x,y)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Server:&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    fn(x, y) {&#xA;        compute&#xA;        return z&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;RPC 的目标是这样的水平透明&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Go example &lt;a href=&#34;https://pdos.csail.mit.edu/6.824&#34;&gt;kv.go&lt;/a&gt;&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;client &amp;ldquo;dial&amp;rdquo; 向 server 端请求调用就像寻常函数调用一样&lt;/li&gt;&#xA;&lt;li&gt;server 并发的处理每一个请求，当然，对于keyvalue要用到锁&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;RPC消息流程图：&lt;/h2&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Client             Server&#xA;&#x9;request---&amp;gt;&#xA;   &#x9;&#x9;&amp;lt;---response&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;软件架构&lt;/h2&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;client app         handlers&#xA;&#x9;stubs           dispatcher&#xA;RPC lib           RPC lib&#xA; 　　net  ------------ net&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;一些细节&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;应该调用哪个服务器函数（handler）？&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;去掉用 Call() 中指定的函数&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;序列化数据： 数列换数据到包中&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;棘手的数组，指针，对象等。&lt;/li&gt;&#xA;&lt;li&gt;Go的RPC库非常强大。&lt;/li&gt;&#xA;&lt;li&gt;有些东西你不能传递：比如channels和function。&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;绑定：client 如何知道该和谁交互&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;client 也许使用 server host name&lt;/li&gt;&#xA;&lt;li&gt;也许使用命名服务，将服务名字映射到最好的服务器。&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;RPC 问题： 当遇到失败会做一些什么操作？&lt;/h2&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;eg: 丢包，网络中断, server 响应缓慢，server 端挂掉&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;错误对RPC客户端意味着什么?&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;client 永远不会收到 server 的响应&lt;/li&gt;&#xA;&lt;li&gt;clinet 不知道 server 是否收到请求(可能在server 发送响应的时候网络中断)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;简单的方案：“最少一次” 执行&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;RPC client 等待响应一定时间，在这段时间内没有收到响应，则重新发送请求，持续这样的操作一定次数后，依然吗没有响应，则向应用汇报错误&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;Q: &amp;ldquo;至少一次&amp;rdquo;容易被应用程序处理吗？&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;至少一次写的简单问题： 客户端发送&amp;rdquo;deduct $10 from bank account&amp;rdquo;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;Q: 客户端可能出现什么错误？&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Put(&amp;ldquo;k&amp;rdquo;,10) &amp;ndash; 一个RPC调用在数据库服务器中设置键值对。&lt;/li&gt;&#xA;&lt;li&gt;Put(&amp;ldquo;k&amp;rdquo;,20) &amp;ndash; 客户端对同一个键设置其他值。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;Q: &amp;ldquo;至少一次&amp;rdquo; 是否是正确的？&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;如果只是读操作没有问题&lt;/li&gt;&#xA;&lt;li&gt;如果应用对于重复写做了处理，也是OK 的&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;更好的RPC行为：&amp;rdquo;最多一次&amp;rdquo;&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;idea:服务器的RPC代码发现重复的请求，返回之前的回复，而不是重写运行。&lt;/li&gt;&#xA;&lt;li&gt;Q：如何发现相同的请求?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;client让每一个请求带有唯一标示码XID(unique ID),相同请求使用相同的XID重新发送。&#xA;server：&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;if seen[xid]:&#xA;    r = old[xid]&#xA;else&#xA;    r = handler()&#xA;    old[xid] = r&#xA;    seen[xid] = true&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;&amp;ldquo;最多一次&amp;rdquo; 的复杂度&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;如何确保 XID 是唯一的？&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;很大的随机数?&lt;/li&gt;&#xA;&lt;li&gt;将唯一的客户端ID（ip address？）和序列号组合起来？&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;server 最后必须丢弃老的 RPC 信息&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;什么时候的确是安全的？&lt;/li&gt;&#xA;&lt;li&gt;想法：&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;唯一的client  IDs&lt;/li&gt;&#xA;&lt;li&gt;前一个rpc请求的序列号&lt;/li&gt;&#xA;&lt;li&gt;客户端每个 RPC 请求都包括 &amp;ldquo;seen all replies &amp;lt;= x&amp;rdquo;&lt;/li&gt;&#xA;&lt;li&gt;类似tcp中的seq和ack&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;或者每次只允许一个RPC调用，到达的是seq+1，那么忽略其他小于seq&lt;/li&gt;&#xA;&lt;li&gt;客户端最多可以尝试5次，服务器会忽略大于5次的请求&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;当原来的请求还在执行，怎么样处理相同seq的请求？&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;服务器不想运行两次，也不想回复。&lt;/li&gt;&#xA;&lt;li&gt;想法：给每个执行的RPC，pending标识；等待或者忽略。&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;如果一个 &amp;ldquo;最多一次&amp;rdquo; 的server挂掉了或是重启了肿么办？&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;如果服务器将副本信息保存在内存中，服务器会忘记请求，同时在重启之后接受相同的请求。&lt;/li&gt;&#xA;&lt;li&gt;也许，你应该将副本信息保存到磁盘？&lt;/li&gt;&#xA;&lt;li&gt;也许，副本服务器应该保存副本信息？&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;&amp;ldquo;至少执行一次&amp;rdquo; 如何？&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;至多一次+无限重试+容错服务&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;Go RPC实现的”最多一次“？&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;打开TCP连接&lt;/li&gt;&#xA;&lt;li&gt;向TCP连接写入请求&lt;/li&gt;&#xA;&lt;li&gt;TCP也许会重传，但是服务器的TCP协议栈会过滤重复的信息&lt;/li&gt;&#xA;&lt;li&gt;在Go代码里面不会有重试（即：不会创建第二个TCP连接）&lt;/li&gt;&#xA;&lt;li&gt;Go RPC代码当没有获取到回复之后将返回错误&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;也许是TCP连接的超时&lt;/li&gt;&#xA;&lt;li&gt;也许是服务器没有看到请求&lt;/li&gt;&#xA;&lt;li&gt;也许服务器处理了请求，但是在返回回复之前服务器的网络故障&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;参考&lt;/h2&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://pdos.csail.mit.edu/6.824/notes/l-rpc.txt&#34;&gt;MIT-8.624 nodes&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/feixiao/Distributed-Systems&#34;&gt;Distributed-Systems&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;</content>
    <link href="http://int64.me/2017/RPC in GO - MIT-6.824.html"></link>
    <author>
      <name>cwen</name>
    </author>
  </entry>
  <entry>
    <title>MapReduce 笔记 - MIT-6.824</title>
    <updated>2017-08-04T00:00:00Z</updated>
    <id>tag:int64.me,2017-08-04:/2017/MapReduce 笔记 - MIT-6.824.html</id>
    <content type="html">&lt;p&gt;MapReduce 由google提出的软件架构，主要用于大规模数据集的并行计算&amp;hellip;&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;MapReduce 概念&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;背景：在几个小时内处理晚TB级别的数据量，eg: 分析一个爬行网也的图形结构，由非分布式系统专家开发的程序运行成千的机器上会是一件很痛苦的事情， eg：错误处理&lt;/li&gt;&#xA;&lt;li&gt;总体目标：非专业程序员可以轻松的在合理的效率下解决的巨大的数据处理问题。程序员定义Map函数和Reduce函数、顺序代码一般都比较简单。 MR在成千的机器上面运行处理大量的数据输入，隐藏全部分布式的细节。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;MapReduce 抽象&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;输入是被切分成 M 个分片&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Input1 -&amp;gt; Map -&amp;gt; a,1   b,1  c,1&#xA;Input2 -&amp;gt; Map -&amp;gt;     b,1&#xA;Input3 -&amp;gt; Map -&amp;gt; a,1      c,1&#xA;                  |   |   |&#xA;                  |   |    -&amp;gt; Reduce -&amp;gt; c,2&#xA;                  |    ----&amp;gt; Reduce -&amp;gt; b,2&#xA;                    ------&amp;gt; Reduce -&amp;gt; a,2&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;MR 在每个输入分片上调用 Map() 函数，产生(k2, v2)这样的中间数据集。  每一个Map()函数的调用就是一个 &amp;ldquo;task&amp;rdquo;&#xA;MR 收集所有key为k2的所有值, 并且把他们传递给Reduce 调用， 最后Reduce输出是这样&lt;k2, v3&gt;这样的一对数值，并存储到输出文件中。&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Example: 单词计算&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;输入是成千文本文件&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; Map(k, v)&#xA;    split v into words&#xA;    for each word w&#xA;      emit(w, &amp;quot;1&amp;quot;)&#xA;  Reduce(k, v)&#xA;    emit(len(v))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;MapReduce 隐藏了很多让人痛苦的细节&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;在服务器上启动软件(s/w)&lt;/li&gt;&#xA;&lt;li&gt;跟踪哪些&amp;rdquo;tasks&amp;rdquo;已经完成&lt;/li&gt;&#xA;&lt;li&gt;数据传送&lt;/li&gt;&#xA;&lt;li&gt;失败恢复&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;MapReduce 易拓展&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;N 台计算机具有 Nx 的吞吐量，假设 M 和 R 都是 大于等于N (即，大量的输入文件和输出的keys), 因为每个Map() 互不影响，所用Map() 函数可以并发的执行。 Reduce() 同样如此。&#xA;所以我们可以通过买更多的电脑来增加吞吐。&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;什么会限制性能？&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;这是我们关系去优化的地方？ CPU? memory? disk? network?&#xA;MepReduce 的作者在 2004 时候，网络带宽是个大问题。关键所有Map()， Reduce() 交互过程中的所有数据都是经过网络的，网络速度是远小于磁盘和内存的速度。 所有当初作者尽量减少网络搬迁数据（如今网络速度相比2004年快了很多)&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;更多细节&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;master: 分发&amp;rdquo;task&amp;rdquo;给 workers;&lt;/li&gt;&#xA;&lt;li&gt;记录m Map task 的中间输出文件、r Reduce&lt;/li&gt;&#xA;&lt;li&gt;输入文件是被存储在 GFS，每个Map 输出文件都有三份;&lt;/li&gt;&#xA;&lt;li&gt;所有计算机都同时运行这GFS 和 MR workers;&lt;/li&gt;&#xA;&lt;li&gt;输入文件是比 workers 多；&lt;/li&gt;&#xA;&lt;li&gt;master 给每个 worker 一个 map task, 只有当老的 task 完成后 master 才会分发新的任务&lt;/li&gt;&#xA;&lt;li&gt;map worker 在本地磁盘上使用 hash 算法将 中间 key 分成 R 份&lt;/li&gt;&#xA;&lt;li&gt;直达所有的Map tasks 全部完成，才会调用 Reduce&lt;/li&gt;&#xA;&lt;li&gt;master 通知 Reducers 从 Map workers 去回去中间数据分区&lt;/li&gt;&#xA;&lt;li&gt;Reduce worker 将最终结果写入GFS(每个 Reduce task 产生一个 文件)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;如何设计去减少慢网络的影响&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Map worker 的输入是从本地磁盘上的GFS副本读取，不经过网络&lt;/li&gt;&#xA;&lt;li&gt;中间数据只经过一次网络&lt;/li&gt;&#xA;&lt;li&gt;Map worker 写数据到本地磁盘，而不是 GFS&lt;/li&gt;&#xA;&lt;li&gt;每个中间数据切分成的文件中都包含许多keys&lt;/li&gt;&#xA;&lt;li&gt;大的网络传输是更加有效率&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;如何更好的负载均衡&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;让n-1 servers 去等待一个server 的结束，这是非常糟糕的， 但是总有 tasks 是别其他的tasks 运行的时间要久。&#xA;解决办法： tasks 的数量要比 workers 多。 master 检测到 workers 的老的tasks 执行结束后， 给他分配新的 task。所以没有比这个 worker自己所能允许的最大时间还长的task存在(希望这样) 。所以运行速度快的 servers 比运行速度慢的servers做更多的工作，但是能在同时完成&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;worker 失败恢复的细节(如何做容错 ）&lt;/h2&gt;&#xA;&#xA;&lt;h3&gt;Map worker 崩溃&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;master 检查到 worker 不在相应心跳&lt;/li&gt;&#xA;&lt;li&gt;崩溃的 worker 的 map 中间输出数据丢失，但是可能每一个 Reduce task 都需要这个数据&lt;/li&gt;&#xA;&lt;li&gt;master 重新调度，在GFS拥有输入文件的其他副本的计算机上重新启动 task&lt;/li&gt;&#xA;&lt;li&gt;有些 Reduce worker 可能已经拥有了读到了崩溃掉机器上的中间数据，在这里我们依赖 map 函数的功能和确定性&lt;/li&gt;&#xA;&lt;li&gt;如果Reduce 以及获取到所有中间数据，那么master不需要重新运行 Map&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h3&gt;Reduce worker 崩溃&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;已经执行结束的 Tasks 是没有问题的- 别存储到了 GFS，带有多个副本&lt;/li&gt;&#xA;&lt;li&gt;master 在其他 workers 上重启崩溃掉的 worker 没有完成的 task&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h3&gt;Reduce worker 在正在写他的输出文件的时候崩溃掉&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;GFS会自动重命名输出，然后使其保持不可见直到Reduce完成，所以master在其他地方再次运行Reduce worker将会是安全的。&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;其他错误和问题&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;如果 master 给两个 workers 同样的 Map() task 肿么办？&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;可能 master 错误的认为其中一个 worker 以及死亡， 它只会告诉Reduce worker其中的一个&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;如果 master 给两个 workers 同样的 Reducer() task 肿么办？&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;它们都会将同一份数据写到GFS上面，GFS的原子重命名操作会触发，先完成的获胜将结果写到GFS.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;如果某一个单独的 worker 是非常的慢 &amp;ndash; 一个掉队者？&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;产生原因可能是非常糟糕的硬件设施。 master会对这些最后的任务创建第二份拷贝任务执行。&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;如果由于硬件或是软件原因造成一个worker计算出一个错误的输出肿么办？&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;太糟糕了！MR假设是建立在&amp;rdquo;fail-stop&amp;rdquo;的cpu和软件之上。&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;如果 master 崩溃了肿么办？&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;单独的 master 挂了， 那就挂了&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;关于那些MapReduce不能很好执行的应用&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;并不是所以工作都适合map/shuffle/reduce这种模式&lt;/li&gt;&#xA;&lt;li&gt;小的数据，因为管理成本太高,如非网站后端&lt;/li&gt;&#xA;&lt;li&gt;大数据中的小更新，比如添加一些文件到大的索引&lt;/li&gt;&#xA;&lt;li&gt;不可预知的读(Map 和 Reduce都不能选择输入)&lt;/li&gt;&#xA;&lt;li&gt;Multiple shuffles, e.g. page-rank (can use multiple MR but not very efficient)&lt;/li&gt;&#xA;&lt;li&gt;多数灵活的系统允许MR，但是使用非常复杂的r模型&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;总结&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;MapReduce 的出现使得大数据计算变得流行起来&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;不是最有效或是灵活的&lt;/li&gt;&#xA;&lt;li&gt;拓展性好&lt;/li&gt;&#xA;&lt;li&gt;容易编程 &amp;ndash; 失败和数据迁移被隐藏起来&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;参考&lt;/h2&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://pdos.csail.mit.edu/6.824/notes/l01.txt&#34;&gt;MIT-8.624 nodes&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://pdos.csail.mit.edu/6.824/papers/mapreduce.pdf&#34;&gt;MapReduce: Simplified Data Processing on Large Clusters&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;</content>
    <link href="http://int64.me/2017/MapReduce 笔记 - MIT-6.824.html"></link>
    <author>
      <name>cwen</name>
    </author>
  </entry>
</feed>